\chapter{Rényi divergences}

\begin{definition}[Rényi divergence]
  \label{def:Renyi}
  %\lean{}
  %\leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$. The Rényi divergence of order $\alpha \in (0,+\infty) \backslash \{1\}$ between $\mu$ and $\nu$ is
  \begin{align*}
  R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1}\log \nu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha\right] \: .
  \end{align*}
\end{definition}

\begin{lemma}
  \label{lem:renyi_eq_log_fDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:Renyi}
  $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - 1)$, which is convex with $f(1)=0$. It is thus a monotone transformation of a f-divergence.

  TODO: use this for the definition?
\end{lemma}

\begin{proof}
Unfold the definitions.
\end{proof}

\begin{definition}[Conditional Rényi divergence]
  \label{def:condRenyi}
  %\lean{}
  %\leanok
  \uses{def:condFDiv}
  Let $\mu$ be a measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels. The conditional Rényi divergence of order $\alpha \in (0,+\infty) \backslash \{1\}$ between $\kappa$ and $\eta$ conditionally to $\mu$ is
  \begin{align*}
  R_\alpha(\kappa, \eta \mid \mu) =\frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\kappa, \eta \mid \mu)) \: ,
  \end{align*}
  for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - 1)$.
\end{definition}

\section{Properties inherited from f-divergences}

Since Rényi divergences are monotone transfomations of f-divergences, every inequality for f-divergences can be translated to Rényi divergences.

\begin{theorem}[Data-processing]
  \label{thm:renyi_data_proc}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $R_\alpha(\kappa \circ \mu, \kappa \circ \nu) \le R_\alpha(\mu, \nu)$.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_data_proc, lem:renyi_eq_log_fDiv}
By Lemma~\ref{lem:renyi_eq_log_fDiv}, $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - 1)$.
The function $x \mapsto \frac{1}{\alpha - 1}\log (1 + (\alpha - 1)x)$ is non-decreasing and $D_f$ satisfies the DPI (Theorem~\ref{thm:fDiv_data_proc}), hence we get the DPI for $R_\alpha$.
\end{proof}

\begin{lemma}
  \label{lem:renyi_data_proc_event}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event. Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$.
  Then $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$.
\end{lemma}

\begin{proof}
\uses{cor:data_proc_event, lem:renyi_eq_log_fDiv}
By Lemma~\ref{lem:renyi_eq_log_fDiv}, $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - 1)$.
By Corollary~\ref{cor:data_proc_event}, $D_f(\mu, \nu) \ge D_f(\mu_E, \nu_E)$, hence $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$.
\end{proof}

\section{Chain rule and tensorization}

\begin{definition}
  \label{def:renyi_measure}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\alpha \in (0, +\infty) \backslash \{1\}$. Let $\xi = \frac{\mu + \nu}{2}$ and $p = \frac{d \mu}{d \xi}$, $q = \frac{d \nu}{d \xi}$. We define a measure $\mu^{(\alpha, \nu)}$, absolutely continuous with respect to $\xi$ with density
  \begin{align*}
  \frac{d \mu^{(\alpha, \nu)}}{d \xi} = p^\alpha q^{1 - \alpha} e^{(\alpha - 1)R_\alpha(\mu, \nu)} \: .
  \end{align*}
\end{definition}

\begin{theorem}[Chain rule]
  \label{thm:renyi_chain_rule}
  %\lean{}
  %\leanok
  \uses{def:renyi_measure, def:condRenyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $R_\alpha(\mu \otimes \kappa, \nu \otimes \eta) = R_\alpha(\mu, \nu) + R_\alpha(\kappa, \eta \mid \mu^{(\alpha, \nu)})$.
\end{theorem}

\begin{proof}
\end{proof}

\begin{corollary}
  \label{cor:renyi_prod_two}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and $\xi, \lambda$ be two measures on $\mathcal Y$.
  Then $R_\alpha(\mu \times \xi, \nu \times \lambda) = R_\alpha(\mu, \nu) + R_\alpha(\xi, \lambda)$.
\end{corollary}

\begin{proof}
\uses{thm:renyi_chain_rule}
Apply Theorem~\ref{thm:renyi_chain_rule} to constant kernels.
\end{proof}

\begin{theorem}[Tensorization]
  \label{thm:renyi_prod}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $I$ be a finite index set. Let $(\mu_i)_{i \in I}, (\nu_i)_{i \in I}$ be measures on measurable spaces $(\mathcal X_i)_{i \in I}$.
  Then $R_\alpha (\prod_{i \in I} \mu_i, \prod_{i \in I} \nu_i) = \sum_{i \in I} R_\alpha(\mu_i, \nu_i)$.
\end{theorem}

\begin{proof}
\uses{cor:renyi_prod_two}
Induction over the finite index set, using Corollary~\ref{cor:renyi_prod_two}.
\end{proof}

\begin{corollary}
  \label{lem:renyi_prod_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$. Let $n \in \mathbb{N}$ and write $\mu^{\otimes n}$ for the product measure on $\mathcal X^n$ of $n$ times $\mu$.
  Then $R_\alpha(\mu^{\otimes n}, \nu^{\otimes n}) = n R_\alpha(\mu, \nu)$.
\end{corollary}

\begin{proof}
\uses{thm:renyi_prod}
Apply Theorem~\ref{thm:renyi_prod}.
\end{proof}