\chapter{Rényi divergences}

\begin{definition}[Rényi divergence]
  \label{def:Renyi}
  %\lean{}
  %\leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$. The Rényi divergence of order $\alpha \in (0,+\infty) \backslash \{1\}$ between $\mu$ and $\nu$ is
  \begin{align*}
  R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1}\log \nu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha\right] \: .
  \end{align*}
\end{definition}

\begin{lemma}
  \label{lem:renyi_eq_log_fDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:Renyi}
  $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - \alpha x + \alpha - 1)$. It is thus a monotone transformation of a f-divergence.

  TODO: use this for the definition?
\end{lemma}

\begin{proof}
Unfold the definitions.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:renyi_data_proc}
  %\lean{}
  %\leanok
  \uses{def:Renyi, thm:fDiv_data_proc}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $R_\alpha(\kappa \circ \mu, \kappa \circ \nu) \le R_\alpha(\mu, \nu)$.
\end{theorem}

\begin{proof}
By Lemma~\ref{lem:renyi_eq_log_fDiv}, $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - \alpha x + \alpha - 1)$.
The function $x \mapsto \frac{1}{\alpha - 1}\log (1 + (\alpha - 1)x)$ is non-decreasing and $D_f$ satisfies the DPI (Theorem~\ref{thm:fDiv_data_proc}), hence we get the DPI for $R_\alpha$.
\end{proof}

\begin{lemma}
  \label{lem:renyi_data_proc_event}
  %\lean{}
  %\leanok
  \uses{cor:data_proc_event}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event. Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$.
  Then $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$.
\end{lemma}

\begin{proof}
By Lemma~\ref{lem:renyi_eq_log_fDiv}, $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - \alpha x + \alpha - 1)$.
By Corollary~\ref{cor:data_proc_event}, $D_f(\mu, \nu) \ge D_f(\mu_E, \nu_E)$, hence $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$.
\end{proof}
