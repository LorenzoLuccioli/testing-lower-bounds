\chapter{Rényi divergences}

\begin{definition}[Rényi divergence]
  \label{def:Renyi}
  %\lean{}
  %\leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$. The Rényi divergence of order $\alpha \in (0,+\infty) \backslash \{1\}$ between $\mu$ and $\nu$ is
  \begin{align*}
  R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1}\log \nu\left[\left(\frac{d \mu}{d \nu}\right)^\alpha\right] \: .
  \end{align*}
\end{definition}

\begin{lemma}
  \label{lem:renyi_eq_log_fDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:Renyi}
  $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - 1)$, which is convex with $f(1)=0$. It is thus a monotone transformation of a f-divergence.

  TODO: use this for the definition?
\end{lemma}

\begin{proof}
Unfold the definitions.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:renyi_data_proc}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $R_\alpha(\kappa \circ \mu, \kappa \circ \nu) \le R_\alpha(\mu, \nu)$.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_data_proc, lem:renyi_eq_log_fDiv}
By Lemma~\ref{lem:renyi_eq_log_fDiv}, $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - \alpha x + \alpha - 1)$.
The function $x \mapsto \frac{1}{\alpha - 1}\log (1 + (\alpha - 1)x)$ is non-decreasing and $D_f$ satisfies the DPI (Theorem~\ref{thm:fDiv_data_proc}), hence we get the DPI for $R_\alpha$.
\end{proof}

\begin{lemma}
  \label{lem:renyi_data_proc_event}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event. Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$.
  Then $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$.
\end{lemma}

\begin{proof}
\uses{cor:data_proc_event, lem:renyi_eq_log_fDiv}
By Lemma~\ref{lem:renyi_eq_log_fDiv}, $R_\alpha(\mu, \nu) = \frac{1}{\alpha - 1} \log (1 + (\alpha - 1) D_f(\nu, \mu))$ for $f : x \mapsto \frac{1}{\alpha - 1}(x^{\alpha} - \alpha x + \alpha - 1)$.
By Corollary~\ref{cor:data_proc_event}, $D_f(\mu, \nu) \ge D_f(\mu_E, \nu_E)$, hence $R_\alpha(\mu, \nu) \ge R_\alpha(\mu_E, \nu_E)$.
\end{proof}

\begin{theorem}[Tensorization]
  \label{thm:renyi_prod}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $I$ be a countable index set (TODO: is countable ok? replace by finite if not). Let $(\mu_i)_{i \in I}, (\nu_i)_{i \in I}$ be measures on measurable spaces $(\mathcal X_i)_{i \in I}$.
  Then $R_\alpha (\prod_{i \in I} \mu_i, \prod_{i \in I} \nu_i) = \sum_{i \in I} R_\alpha(\mu_i, \nu_i)$.
\end{theorem}

\begin{proof}
\end{proof}

\begin{corollary}
  \label{lem:renyi_prod_n}
  %\lean{}
  %\leanok
  \uses{def:Renyi}
  Let $\mu, \nu$ be two measures on $\mathcal X$. Let $n \in \mathbb{N}$ and write $\mu^{\otimes n}$ for the product measure on $\mathcal X^n$ of $n$ times $\mu$.
  Then $R_\alpha(\mu^{\otimes n}, \nu^{\otimes n}) = n R_\alpha(\mu, \nu)$.
\end{corollary}

\begin{proof}
\uses{thm:renyi_prod}
Apply Theorem~\ref{thm:renyi_prod}.
\end{proof}