\chapter{Kullback-Leibler divergence}

\begin{definition}[Kullback-Leibler divergence]
  \label{def:KL}
  \lean{ProbabilityTheory.kl}
  \leanok
  %\uses{}
  Let $\mu, \nu$ be two measures on $\mathcal X$. The Kullback-Leibler divergence between $\mu$ and $\nu$ is
  \begin{align*}
  \KL(\mu, \nu) = \mu\left[\log \frac{d \mu}{d \nu}\right]
  \end{align*}
  if $\mu \ll \nu$ and $x \mapsto \log \frac{d \mu}{d \nu}(x)$ is $\mu$-integrable and $+\infty$ otherwise.
\end{definition}

\begin{lemma}
  \label{lem:kl_eq_fDiv}
  \lean{ProbabilityTheory.kl_eq_fDiv}
  \leanok
  \uses{def:KL, def:fDiv}
  $\KL(\mu, \nu) = D_f(\mu, \nu)$ for $f: x \mapsto x \log x$.
\end{lemma}

\begin{proof}\leanok
Simple computation.
\end{proof}

\begin{definition}[Conditional Kullback-Leibler divergence]
  \label{def:condKL}
  \lean{ProbabilityTheory.condKL}
  \leanok
  \uses{def:KL}
  Let $\mu$ be a measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels. The Kullback-Leibler divergence between $\kappa$ and $\eta$ conditionally to $\mu$ is
  \begin{align*}
    \KL(\kappa, \eta \mid \mu) = D_f(\kappa, \eta \mid \mu)
  \end{align*}
  with $f: x \mapsto x \log x$.
\end{definition}

\begin{lemma}
  \label{lem:condKL_eq_condFDiv}
  \lean{ProbabilityTheory.kl_eq_fDiv}
  \leanok
  \uses{def:condKL, def:condFDiv}
  $\KL(\kappa, \eta \mid \mu) = D_f(\kappa, \eta \mid \mu)$ for $f: x \mapsto x \log x$.
\end{lemma}

\begin{proof}\leanok
Simple computation.
\end{proof}

\section{Properties inherited from f-divergences}

Since $\KL$ is an f-divergence, every inequality for f-divergences can be translated to $\KL$.

\begin{lemma}
  \label{lem:kl_self}
  \lean{ProbabilityTheory.kl_self}
  \leanok
  \uses{def:KL}
  $KL(\mu, \mu) = 0$.
\end{lemma}

\begin{proof} \leanok
  \uses{lem:kl_eq_fDiv, lem:fDiv_self}
  KL is an f-divergence thanks to Lemma \ref{lem:kl_eq_fDiv}, then apply Lemma \ref{lem:fDiv_self}.
\end{proof}

\begin{theorem}[Marginals]
  \label{thm:kl_fst_le}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X \times \mathcal Y$ where $\mathcal Y$ is standard Borel, and let $\mu_X, \nu_X$ be their marginals on $\mathcal X$.
  Then $\KL(\mu_X, \nu_X) \le \KL(\mu, \nu)$.
  Similarly, for $\mathcal X$ standard Borel and $\mu_Y, \nu_Y$ the marginals on $\mathcal Y$, $\KL(\mu_Y, \nu_Y) \le \KL(\mu, \nu)$.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_fst_le, lem:kl_eq_fDiv}
Apply Theorem~\ref{thm:fDiv_fst_le}.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:kl_data_proc}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $\KL(\kappa \circ \mu, \kappa \circ \nu) \le \KL(\mu, \nu)$.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_data_proc, lem:kl_eq_fDiv}
Apply Theorem~\ref{thm:fDiv_data_proc}.
\end{proof}

\begin{lemma}
  \label{lem:kl_ge}
  \lean{ProbabilityTheory.kl_ge_mul_log}
  \leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$. Then $\KL(\mu, \nu) \ge \mu(\mathcal X) \log \frac{\mu(\mathcal X)}{\nu(\mathcal X)}$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:le_fDiv, lem:kl_eq_fDiv}
\end{proof}

\begin{lemma}
  \label{lem:kl_nonneg}
  \lean{ProbabilityTheory.kl_nonneg}
  \leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two probability measures. Then $\KL(\mu, \nu) \ge 0$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:kl_ge}
Apply Lemma~\ref{lem:kl_ge} and use $\mu(\mathcal X) = \nu (\mathcal X)$.
\end{proof}

\begin{lemma}
  \label{lem:kl_convex}
  %\lean{}
  %\leanok
  \uses{def:KL}
  $(\mu, \nu) \mapsto \KL(\mu, \nu)$ is convex.
\end{lemma}

\begin{proof}
\uses{thm:fDiv_convex, lem:kl_eq_fDiv}
Apply Theorem~\ref{thm:fDiv_convex}
\end{proof}

\section{Chain rule and tensorization}

\begin{theorem}[Chain rule, kernel version]
  \label{thm:kl_chain_rule}
  %\lean{}
  %\leanok
  \uses{def:KL, def:condKL}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ two Markov kernels.
  Then $\KL(\mu \otimes \kappa, \nu \otimes \eta) = \KL(\mu, \nu) + \KL(\kappa, \eta \mid \mu)$.
\end{theorem}

\begin{proof}
\uses{lem:rnDeriv_compProd}
Use Lemma~\ref{lem:rnDeriv_compProd} and Corollary~\ref{cor:rnDeriv_value} in a computation:
\begin{align*}
\KL(\mu \otimes \kappa, \nu \otimes \eta)
&= \int_p \log \frac{d(\mu \otimes \kappa)}{d(\nu \otimes \eta)}(p) \partial (\mu \otimes \kappa)
\\
&= \int_x \int_y\log \left(\frac{d\mu}{d \nu}(x)\frac{d\kappa}{d \eta}(x,y)\right) \partial \kappa(x) \partial\mu
\\
&= \int_x \int_y\log \left(\frac{d\mu}{d \nu}(x)\right) + \log \left(\frac{d\kappa}{d \eta}(x,y)\right) \partial \kappa(x) \partial\mu
\\
&= \int_x \log \left(\frac{d\mu}{d \nu}(x)\right)\partial\mu + \int_x \int_y\log \left(\frac{d\kappa}{d \eta}(x,y)\right) \partial \kappa(x) \partial\mu
\\
&= \int_x \log \left(\frac{d\mu}{d \nu}(x)\right)\partial\mu + \int_x \int_y\log \left(\frac{d\kappa(x)}{d \eta(x)}(y)\right) \partial \kappa(x) \partial\mu
\\
&= \KL(\mu, \nu) + \KL(\kappa, \eta \mid \mu)
\: .
\end{align*}

\end{proof}

\begin{theorem}[Chain rule, product version]
  \label{thm:kl_chain_rule_prod}
  %\lean{}
  %\leanok
  \uses{def:KL, def:condKL}
  Let $\mu, \nu$ be two measures on $\mathcal X \times \mathcal Y$, where $\mathcal Y$ is standard Borel.
  Then $\KL(\mu, \nu) = \KL(\mu_X, \nu_X) + \KL(\mu_{Y|X}, \nu_{Y|X} \mid \mu_X)$.
\end{theorem}

\begin{proof}
\uses{thm:kl_chain_rule}
Write $\mu = \mu_X \otimes \mu_{Y|X}$ and $\nu = \nu_X \otimes \nu_{Y|X}$, then use Theorem~\ref{thm:kl_chain_rule}.
\end{proof}

\begin{lemma}
  \label{lem:kl_chain_rule_cond_event}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two measures and $E$ an event. Let $\mu_{|E}$ be the measure defined by $\mu_{|E}(A) = \frac{\mu(A \cap E)}{\mu(E)}$ and define $\nu_{|E}$, $\mu_{| E^c}$ and $\nu_{| E^c}$ similarly. Let $\kl(p,q) = p\log\frac{p}{q} + (1-p)\log\frac{1-p}{1-q}$ be the Kullback-Leibler divergence between Bernoulli distributions with means $p$ and $q$. Then
  \begin{align*}
  \KL(\mu, \nu) \ge \kl(\mu(E), \nu(E)) + \mu(E) \KL(\mu_{|E}, \nu_{|E}) + \mu(E^c) \KL(\mu_{|E^c}, \nu_{|E^c}) \: .
  \end{align*}
\end{lemma}

\begin{proof}
\uses{thm:kl_chain_rule, lem:fDiv_compProd_prod_eq}
Let $\mu_E$ be the Bernoulli distribution with $\mu_E(\{1\}) = \mu(E)$, and define $\nu_E$ similarly.
Let $\kappa : \{0,1\} \rightsquigarrow \mathcal X$ be the kernel defined by $\kappa(1) = \mu_{|E}$ and $\kappa(0) = \mu_{|E^c}$. Define a kernel $\eta$ similarly for $\nu$.
Then $\mu = \kappa \circ \mu_E$ and $\nu = \eta \circ \nu_E$.

$\mu_E \otimes \kappa$ is equal to the composition of $\mu$ and the deterministic kernel given by the function $f : x \mapsto (\mathbb{I}_E(x), x)$. By Lemma~\ref{lem:fDiv_compProd_prod_eq}, that gives $\KL(\mu_E \otimes \kappa, \nu_E \otimes \eta) = \KL(\mu, \nu)$.

Using the chain rule (Theorem~\ref{thm:kl_chain_rule}),
\begin{align*}
\KL(\mu_E \otimes \kappa, \nu_E \otimes \eta)
&= \KL(\mu_E, \nu_E) + \KL(\kappa, \eta \mid \mu_E)
\\
&= \KL(\mu_E, \nu_E) + \mu(E) \KL(\mu_{|E}, \nu_{|E}) + \mu(E^c) \KL(\mu_{|E^c}, \nu_{|E^c})
\: .
\end{align*}

\end{proof}

\begin{lemma}
  \label{lem:expectation_llr_event}
  %\lean{}
  %\leanok
  \uses{def:KL}
  Let $\mu, \nu$ be two measures and $E$ an event. Then
  $\mu(E)\log\frac{\mu(E)}{\nu(E)} \le \mu\left[\mathbb{I}(E)\log \frac{d \mu}{d \nu}\right]$ .
\end{lemma}

\begin{proof}
\uses{lem:kl_ge}
Apply Lemma~\ref{lem:kl_ge} to the measures $\mu$ and $\nu$ restricted to $E$. $\mu\left[\mathbb{I}(E)\log \frac{d \mu}{d \nu}\right]$ is the KL between those two distributions.
\end{proof}