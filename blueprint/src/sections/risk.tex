\chapter{Testing, Estimation, Risk}

In the estimation problem, we are given a family of measures and we want to estimate an unknown parameter for the measure from which we observe samples.
Let $\mathcal X$ be the measurable space on which the measures are defined and let $y : \mathcal M(\mathcal X) \to \mathcal Y$ be the function which maps a measure to the parameter of interest.

An estimator is a Markov kernel $\hat{y} : \mathcal X \rightsquigarrow \mathcal Z$, where most often $\mathcal Z = \mathcal Y$.
Note that while $y$ is a function of a measure in $\mathcal M(\mathcal X)$, $\hat{y}$ has domain $\mathcal X$ as the estimation is done based on a sample of the measure, while the measure itself remains unknown.
In general this is a randomized estimator, but often deterministic estimators (corresponding to deterministic kernels) are enough to attain optimal performance.
The quality of the estimation is quantified by a loss $\ell' : \mathcal Y \times \mathcal Z \to \mathbb{R}_{+, \infty}$. We write $\ell$ for a loss from $\mathcal Y \times \mathcal Y$ and $\ell'$ in the heterogeneous case.

The measure family is described by a kernel $P : \Theta \rightsquigarrow \mathcal X$. We write $P_\theta$ for $P(\theta)$ and $y(\theta)$ for $y(P_\theta)$.
In \emph{parametric} problems, $\Theta$ is a low-dimensional space. In \emph{nonparametric} problems, $\Theta$ is large, for example $\Theta = \mathcal M (\mathcal X)$ or $\Theta = \mathcal P(\mathcal X)$.

TODO: reorder to define $y: \Theta \to \mathcal Y$ instead of $y : \mathcal M(\mathcal X) \to \mathcal Y$~, since we always write $y(P(\theta))$.

\paragraph{Example: simple binary hypothesis testing (SBHT)}

We say that an estimation problem is a testing problem if $\mathcal Y = \mathcal Z$ is discrete and the loss takes values in $\{0, 1\}$.
A test is said to be binary if $\mathcal Y$ has two elements.
A test is simple if $\{\theta \mid y(P_\theta) = y_0\}$ is a singleton for all $y_0 \in \mathcal Y$.

In summary, in simple binary hypothesis testing, $\Theta = \mathcal Y = \mathcal Z = \{0,1\}$, $y(P_0) = 0$, $y(P_1) = 1$ (that is, $y(\theta) = \theta$). For $z \in \{0,1\}$, $\ell(y, z) = \mathbb{I}\{y \ne z\}$.

\section{Risk}

Let $\hat{\mu}_\theta$ be the law of $\hat{y}(\theta)$. That is, $\hat{\mu} : \mathcal \Theta \rightsquigarrow \mathcal X$ is the kernel $\hat{y} \circ P$.

\begin{definition}[Risk]
  \label{def:risk}
  %\lean{}
  %\leanok
  %\uses{}
  The risk of an estimator $\hat{y}$ at $\theta \in \Theta$ is $r_\theta(\hat{y}) = \hat{\mu}_\theta\left[z \mapsto \ell'(y(\theta), z)\right]$~.
\end{definition}

\emph{Example (SBHT):} $r_\theta(\hat{y}) = \hat{\mu}_\theta(\hat{y}(\theta) \ne y(\theta))$.


\begin{definition}[Bayesian risk]
  \label{def:bayesianRisk}
  %\lean{}
  %\leanok
  \uses{def:risk}
  The Bayesian risk of an estimator $\hat{y}$ for a prior $\pi \in \mathcal P(\Theta)$ is $R_\pi(\hat{y}) = \pi\left[\theta \mapsto r_\theta(\hat{y})\right]$~.
\end{definition}


\begin{definition}[Bayes risk]
  \label{def:bayesRisk}
  %\lean{}
  %\leanok
  \uses{def:bayesianRisk}
  The Bayes risk for prior $\pi \in \mathcal P(\Theta)$ is $\mathcal R_\pi = \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} R_\pi(\hat{y})$~.

  The Bayes risk of $P$ is $\mathcal R^*_B = \sup_{\pi \in \mathcal P(\Theta)} \mathcal R_\pi \: .$
\end{definition}

\emph{Example (SBHT):} $\mathcal R^*_B = \sup_{\gamma \in [0,1]}\inf_{\hat{y}}\left(\gamma \hat{\mu}_0(\hat{y} \ne 0) + (1 - \gamma) \hat{\mu}_1(\hat{y} \ne 1)\right)$.

\begin{definition}[Bayes estimator]
  \label{def:bayesEstimator}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  An estimator $\hat{y}$ is said to be a Bayes estimator for a prior $\pi \in \mathcal P(\Theta)$ if $R_\pi(\hat{y}) = \mathcal R_\pi$.
\end{definition}

\begin{definition}[Generalized Bayes estimator]
  \label{def:genBayesEstimator}
  %\lean{}
  %\leanok
  \uses{def:risk,def:bayesInv}
  The generalized Bayes estimator for prior $\pi \in \mathcal P(\Theta)$ and kernel $P: \Theta \rightsquigarrow \mathcal X$ is the deterministic estimator $\mathcal X \to \mathcal Z$ given by
  \begin{align*}
  x \mapsto \arg\min_z P_\pi^\dagger(x)\left[\theta \mapsto \ell'(y(\theta), z)\right] \: .
  \end{align*}
  Here $P_\pi^\dagger$ is the Bayesian inverse of $P$ with respect to $\pi$ (that is, $P_\pi^\dagger(x)$ is the posterior distribution of $\theta$ given $x$).
\end{definition}

\begin{definition}[Minimax risk]
  \label{def:minimaxRisk}
  %\lean{}
  %\leanok
  \uses{def:risk}
  The minimax risk of $P$ is $\mathcal R^* = \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} \sup_{\theta \in \Theta} r_\theta(\hat{y})$~.
\end{definition}

\emph{Example (SBHT):} $\mathcal R^* = \inf_{\hat{y}} \max\{\hat{\mu}_0(\hat{y} \ne 0), \hat{\mu}_1(\hat{y} \ne 1)\}$.

\begin{lemma}
  \label{lem:bayesRisk_le_minimaxRisk}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk, def:minimaxRisk}
  $\mathcal R_B^* \le \mathcal R^*$.
\end{lemma}

\begin{proof}
For any $\pi \in \mathcal P(\mathcal X)$ and any estimator, $\pi\left[\hat{\mu}_\theta\left[\ell'(y(\theta), \hat{y}(\theta))\right]\right] \le \sup_{\theta \in \Theta}\hat{\mu}_\theta\left[\ell'(y(\theta), \hat{y}(\theta))\right]$.
\end{proof}

TODO: ``often'', $\mathcal R^*_B = \mathcal R^*$.

\section{Simple binary hypothesis testing}

\begin{definition}
  \label{def:bayesBinaryRisk}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  The Bayes binary risk between measures $\mu$ and $\nu$, denoted by $B_\pi(\mu, \nu)$ and parametrized by $\pi \in [0,1]$, is the Bayes risk $\mathcal R_\pi$ for $\Theta = \mathcal Y = \mathcal Z = \{0,1\}$, $\ell(y,z) = \mathbb{I}\{y \ne z\}$, $P$ the kernel sending 0 to $\mu$ and 1 to $\nu$ and prior $(\pi, 1-\pi)$.
  That is,
  \begin{align*}
  B_\pi(\mu, \nu) = \inf_{\hat{y} : \mathcal X \rightsquigarrow \{0,1\}}\left(\pi (\hat{y} \circ \mu)(\{1\}) + (1 - \pi) (\hat{y} \circ \nu)(\{0\})\right)
  \: ,
  \end{align*}
  in which the infimum is over Markov kernels.
\end{definition}

\begin{lemma}
  \label{lem:bayesBinaryRisk_le}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk}
  For all probability measures $\mu, \nu$, $B_\pi(\mu, \nu) \le \min\{\pi, 1 - \pi\}$~.
\end{lemma}

\begin{proof}%\leanok
\uses{def:bayesRisk}
Since $B_\pi(\mu, \nu)$ is an infimum over estimators, we can find an upper bound by choosing an estimator. We choose for $\hat{y}$ a constant estimator, with value 1 if $1- \pi > \pi$ and 0 otherwise.
That estimator has Bayesian risk $\min\{\pi, 1 - \pi\}$.
\end{proof}

\begin{lemma}
  \label{lem:bayesBinaryRisk_self}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk}
  For $\mu \in \mathcal P(\mathcal X)$, $B_\pi(\mu, \mu) = \min\{\pi, 1-\pi\}$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_bayesBinaryRisk}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $B_\pi(\kappa \circ \mu, \kappa \circ \nu) \ge B_\pi(\mu, \nu)$~.
\end{theorem}

\begin{proof}%\leanok
\uses{}
\begin{align*}
B_\pi(\kappa \circ \mu, \kappa \circ \nu)
&= \inf_{\hat{y} : \mathcal Y \rightsquigarrow \{0,1\}}\left(\pi (\hat{y} \circ \kappa \circ \mu)(\{1\}) + (1 - \pi) (\hat{y} \circ \kappa \circ \nu)(\{0\})\right)
\\
&\ge \inf_{\hat{y}' : \mathcal X \rightsquigarrow \{0,1\}}\left(\pi (\hat{y}' \circ \mu)(\{1\}) + (1 - \pi) (\hat{y}' \circ \nu)(\{0\})\right)
\\
&= B_\pi(\mu, \nu)
\: .
\end{align*}
The inequality is due to taking an infimum over a larger set: $\{\hat{y} \circ \kappa \mid \hat{y} : \mathcal Y \rightsquigarrow \{0,1\} \text{ Markov}\} \subseteq \{\hat{y}' : \mathcal X \rightsquigarrow \{0,1\} \mid \hat{y} \text{ Markov}\}$~.
\end{proof}

\begin{definition}
  \label{def:deGrootInfo}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk}
  The De Groot statistical information between measures $\mu$ and $\nu$ is $I_\pi(\mu, \nu) = \min\{\pi, 1 - \pi\} - B_\pi(\mu, \nu)$.
\end{definition}

\begin{lemma}
  \label{lem:deGrootInfo_self}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu \in \mathcal P(\mathcal X)$, $I_\pi(\mu, \mu) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_self}
Use Lemma~\ref{lem:bayesBinaryRisk_self}.
\end{proof}

\begin{lemma}
  \label{lem:deGrootInfo_nonneg}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal P(\mathcal X)$, $I_\pi(\mu, \nu) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_le}
Use Lemma~\ref{lem:bayesBinaryRisk_le}.
\end{proof}

\begin{lemma}
  \label{lem:deGrootInfo_half}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo, def:TV}
  For $\mu, \nu \in \mathcal P(\mathcal X)$, $I_{1/2}(\mu, \nu) = \frac{1}{2}\TV(\mu, \nu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_deGrootInfo}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $I_\pi(\kappa \circ \mu, \kappa \circ \nu) \le I_\pi(\mu, \nu)$~.
\end{theorem}

Note that this theorem is valid for any measurable space: no countability is required.

\begin{proof}%\leanok
\uses{thm:data_proc_bayesBinaryRisk}
It suffices to prove $B_\pi(\kappa \circ \mu, \kappa \circ \nu) \ge B_\pi(\mu, \nu)$ since $I_\pi(\mu, \nu) = \min\{\pi, 1 - \pi\} - B_\pi(\mu, \nu)$.
The inequality was proved in Theorem~\ref{thm:data_proc_bayesBinaryRisk}.
\end{proof}

TODO: this extends the data-processing inequality for TV to any space.

TODO: once we prove the integral form of any $f$-divergence with respect to the De Groot statistical information, this will extend the data-processing inequality for all $f$-divergences. 

\begin{definition}
  \label{def:deGrootFun}
  %\lean{}
  %\leanok
  \uses{}
  For $\pi \in (0,1)$ let $\psi_\pi : \mathbb{R} \to \mathbb{R}$ be the function defined by
  \begin{align*}
  \psi_\pi(x) &= \pi \max\left\{0, x - \frac{1 - \pi}{\pi}\right\} & \text{ for } \pi &\le 1/2 \: ,
  \\
  \psi_\pi(x) &= \pi \max\left\{0, \frac{1 - \pi}{\pi} - x\right\} & \text{ for } \pi &> 1/2 \: .
  \end{align*}
\end{definition}

\begin{lemma}
  \label{lem:deGrootInfo_eq_fDiv}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo, def:deGrootFun}
  The De Groot statistical information $I_\pi$ is an $f$-divergence for the function $\psi_\pi$.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

\subsection{Integral representation of $f$-divergences}

\begin{definition}
  \label{def:curvatureMeasure}
  %\lean{}
  %\leanok
  \uses{}
  Let $f: \mathbb{R} \to \mathbb{R}$ be a convex function. Then $f'_+(x) \coloneqq \lim_{y \downarrow x}\frac{f(y) - f(x)}{y - x}$ is a Stieltjes function (a monotone right continuous function) and it defines a measure $\gamma_f$ on $\mathbb{R}$ by $\gamma_f((x,y]) \coloneqq f'_+(y) - f'_+(x)$~. \cite{liese2012phi} calls $\gamma_f$ the curvature measure of $\phi$.
\end{definition}

\begin{lemma}
  \label{lem:convex_taylor_gt}
  %\lean{}
  %\leanok
  \uses{def:curvatureMeasure}
  For $f: \mathbb{R} \to \mathbb{R}$ a convex function and $x,y \in \mathbb{R}$ with $x < y$,
  \begin{align*}
  f(y) - f(x) - (y - x)f'_+(x) = \int_{z \in (x,y]} (y - z) \partial \gamma_f \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

\begin{lemma}
  \label{lem:convex_taylor_lt}
  %\lean{}
  %\leanok
  \uses{def:curvatureMeasure}
  For $f: \mathbb{R} \to \mathbb{R}$ a convex function and $x,y \in \mathbb{R}$ with $y < x$,
  \begin{align*}
  f(y) - f(x) - (y - x)f'_+(x) = \int_{z \in (y,x]} (z - y) \partial \gamma_f \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

TODO: define $\Gamma_f$.

\begin{theorem}[\cite{liese2006divergences,liese2012phi}]
  \label{thm:fDiv_eq_integral}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:curvatureMeasure}
  \begin{align*}
  D_f(\mu, \nu) = f(1) \nu(\mathcal X) + f'_+(1)(\mu(\mathcal X) - \nu(\mathcal X)) + \int_\pi I_\pi(\mu, \nu) \partial\Gamma_f \: .
  \end{align*}
  
\end{theorem}

\begin{proof}%\leanok
\uses{}

\end{proof}

\section{Deficiency}

\begin{definition}[Le Cam deficiency]
  \label{def:deficiency}
  %\lean{}
  %\leanok
  \uses{def:TV}
  The Le Cam deficiency of $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ with respect to $\eta : \mathcal X \rightsquigarrow \mathcal Z$ is
  \begin{align*}
  \delta_{\TV}(\kappa, \eta) = \inf_{\xi} \sup_{x \in \mathcal X} \TV(\xi \circ \kappa (x), \eta(x)) \: ,
  \end{align*}
  in which the infimum is taken over all Markov kernels $\xi : \mathcal Y \rightsquigarrow \mathcal Z$~.
\end{definition}

TODO: from any divergence on measures $D$, \cite{perrone2023markov} defines a divergence between kernels $\kappa, \kappa' : \mathcal X \rightsquigarrow \mathcal Y$ by $D(\kappa, \kappa') = \sup_{x \in \mathcal X} D(\kappa(x), \eta(x))$.
If we see measures as kernels from a point space, the divergence between measures-as-kernels coincides with the divergence between the measures.
With that notation, $\delta_{\TV}(\kappa, \eta) = \inf_{\xi} \TV(\xi \circ \kappa, \eta)$.
We could also proceed as \cite{raginsky2011shannon} and extend the notion of deficiency to deficiency with respect to a divergence: $\delta_D(\kappa, \eta) = \inf_{\xi} D(\eta, \xi\circ\kappa)$ (note the reversed arguments, which does not matter for the case of $\TV$ since it is symmetric).

\begin{lemma}
  \label{lem:deficiency_nonneg}
  %\lean{}
  %\leanok
  \uses{def:deficiency}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$ and $\eta : \mathcal X \rightsquigarrow \mathcal Z$. Then $\delta_{\TV}(\kappa, \eta) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:tv_nonneg}
This is a consequence of Lemma~\ref{lem:tv_nonneg}.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_self}
  %\lean{}
  %\leanok
  \uses{def:deficiency}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$. Then $\delta_{\TV}(\kappa, \kappa) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:deficiency_nonneg}
By Lemma~\ref{lem:deficiency_nonneg}, $0 \le \delta_{\TV}(\kappa, \kappa)$.
In the infimum that defines $\delta_{\TV}(\kappa, \kappa)$, take $\xi = \textup{id}$ to get an upper bound of 0 on $\delta_{\TV}(\kappa, \kappa)$.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_comp}
  %\lean{}
  %\leanok
  \uses{def:deficiency, def:blackwellOrder}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$ and $\eta : \mathcal X \rightsquigarrow \mathcal Z$.
  If $\eta \le_B \kappa$ (that is, there exists a Markov kernel $\chi : \mathcal Y \rightsquigarrow \mathcal Z$ such that $\eta = \chi \circ \kappa$), then $\delta_{\TV}(\kappa, \eta) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:deficiency_nonneg}
In the infimum that defines $\delta_{\TV}(\kappa, \eta)$, take $\xi = \chi$ to get an upper bound of 0 on $\delta_{\TV}(\kappa, \eta)$. The lower bound is Lemma~\ref{lem:deficiency_nonneg}.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_antimono_left}
  %\lean{}
  %\leanok
  \uses{def:deficiency, def:blackwellOrder}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$, $\eta : \mathcal X \rightsquigarrow \mathcal Z$ and $\zeta : \mathcal X \rightsquigarrow \mathcal W$ be Markov kernels.
  If $\eta \le_B \kappa$ then $\delta_{\TV}(\kappa, \zeta) \le \delta_{\TV}(\eta, \zeta)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
Let $\chi$ be a Markov kernel such that $\eta = \chi \circ \kappa$~.
\begin{align*}
\delta_{\TV}(\eta, \zeta)
&= \inf_{\xi} \sup_{x \in \mathcal X} \TV(\xi \circ \eta (x), \zeta(x))
\\
&= \inf_\xi \sup_{x \in \mathcal X} \TV(\xi \circ \chi \circ \kappa (x), \zeta(x))
\: .
\end{align*}
Since $\xi \circ \chi$ is a Markov kernel, the infimum over Markov kernels of the form $\xi \circ \chi$ is larger than the infimum over all Markov kernels.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_mono_right}
  %\lean{}
  %\leanok
  \uses{def:deficiency, def:blackwellOrder}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$, $\eta : \mathcal X \rightsquigarrow \mathcal Z$ and $\zeta : \mathcal X \rightsquigarrow \mathcal W$ be Markov kernels.
  If $\zeta \le_B \eta$ then $\delta_{\TV}(\kappa, \zeta) \le \delta_{\TV}(\kappa, \eta)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:tv_data_proc}
Let $\chi$ be a Markov kernel such that $\zeta = \chi \circ \eta$~.
\begin{align*}
\delta_{\TV}(\kappa, \zeta)
&= \inf_{\xi : \mathcal Y \rightsquigarrow \mathcal W} \sup_{x \in \mathcal X} \TV(\xi \circ \kappa (x), \zeta(x))
\\
&= \inf_{\xi : \mathcal Y \rightsquigarrow \mathcal W} \sup_{x \in \mathcal X} \TV(\xi \circ \kappa (x), \chi \circ \eta(x))
\\
&\le \inf_{\xi' : \mathcal Y \rightsquigarrow \mathcal Z} \sup_{x \in \mathcal X} \TV(\chi \circ \xi' \circ \kappa (x), \chi \circ \eta(x))
\: .
\end{align*}
By the data-processing inequality for $\TV$ (Theorem \ref{thm:tv_data_proc}), for all $\xi' : \mathcal Y \rightsquigarrow \mathcal Z$, $\TV(\chi \circ \xi' \circ \kappa (x), \chi \circ \eta(x)) \le \TV(\xi' \circ \kappa (x), \eta(x))$.
This concludes the proof.
\end{proof}

\section{Reduction to testing}

TODO: lower bounds on estimation by reduction to testing problems.