\chapter{Testing, Estimation, Risk}

In the estimation problem, we are given a family of measures and we want to estimate an unknown parameter for the measure from which we observe samples.
Let $\mathcal X$ be the measurable space on which the measures are defined.
The measure family is described by a kernel $P : \Theta \rightsquigarrow \mathcal X$. We write $P_\theta$ for $P(\theta)$.
In \emph{parametric} problems, $\Theta$ is a low-dimensional space. In \emph{nonparametric} problems $\Theta$ is large, for example $\Theta = \mathcal M (\mathcal X)$ or $\Theta = \mathcal P(\mathcal X)$.

The goal of the estimation task is to find from the observation of samples of $P_\theta$ for an unknown $\theta$ the value of a function $y : \Theta \to \mathcal Y$~, which maps the parameter $\theta$ to the quantity we are interested in. For example, $y(\theta)$ might be the expected value of the distribution $P_\theta$.

An estimator is a Markov kernel $\hat{y} : \mathcal X \rightsquigarrow \mathcal Z$, where most often $\mathcal Z = \mathcal Y$.
Note that while $y$ is a function of a parameter in $\Theta$, $\hat{y}$ has domain $\mathcal X$ as the estimation is done based on a sample of the measure, while the measure itself and its parameter remain unknown.
In general this is a randomized estimator, but often deterministic estimators (corresponding to deterministic kernels) are enough to attain optimal performance.
The quality of the estimation is quantified by a loss $\ell' : \mathcal Y \times \mathcal Z \to \mathbb{R}_{+, \infty}$. We write $\ell$ for a loss on $\mathcal Y \times \mathcal Y$ and $\ell'$ in the heterogeneous case.

In short, an estimation problem is specified by the data $(P, y, \ell')$.

\paragraph{Example: simple binary hypothesis testing (SBHT)}

We say that an estimation problem is a testing problem if $\mathcal Y = \mathcal Z$ is discrete and the loss takes values in $\{0, 1\}$.
A test is said to be binary if $\mathcal Y$ has two elements.
A test is simple if $\{\theta \mid y(\theta) = y_0\}$ is a singleton for all $y_0 \in \mathcal Y$.

In summary, in simple binary hypothesis testing, $\Theta = \mathcal Y = \mathcal Z = \{0,1\}$, $y(0) = 0$, $y(1) = 1$ (that is, $y$ is the identity). For $z \in \{0,1\}$, $\ell(y, z) = \mathbb{I}\{y \ne z\}$.

\section{Risk}

Let $\hat{\mu}_\theta$ be the law of $\hat{y}(\theta)$. That is, $\hat{\mu} : \mathcal \Theta \rightsquigarrow \mathcal X$ is the kernel $\hat{y} \circ P$.

\begin{definition}[Risk]
  \label{def:risk}
  \lean{ProbabilityTheory.risk}
  \leanok
  \uses{def:kernel_comp}
  The risk of an estimator $\hat{y}$ on the estimation problem $(P, y, \ell')$ at $\theta \in \Theta$ is $r^P_\theta(\hat{y}) = \hat{\mu}_\theta\left[z \mapsto \ell'(y(\theta), z)\right]$~.
\end{definition}

\emph{Example (SBHT):} $r^P_\theta(\hat{y}) = \hat{\mu}_\theta(\hat{y}(\theta) \ne y(\theta))$.


\begin{definition}[Bayesian risk]
  \label{def:bayesianRisk}
  \lean{ProbabilityTheory.bayesianRisk}
  \leanok
  \uses{def:risk}
  The Bayesian risk of an estimator $\hat{y}$ on $(P, y, \ell')$ for a prior $\pi \in \mathcal M(\Theta)$ is $R^P_\pi(\hat{y}) = \pi\left[\theta \mapsto r^P_\theta(\hat{y})\right]$~. It can also be expanded as $R^P_\pi(\hat{y}) = (\pi \otimes (\hat{y} \circ P))\left[ (\theta, z) \mapsto \ell'(y(\theta), z) \right]$~.
\end{definition}


\begin{definition}[Bayes risk]
  \label{def:bayesRisk}
  \lean{ProbabilityTheory.bayesRisk}
  \leanok
  \uses{def:bayesianRisk}
  The Bayes risk of $(P, y, \ell')$ for prior $\pi \in \mathcal M(\Theta)$ is $\mathcal R^P_\pi = \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} R^P_\pi(\hat{y})$~, where the infimum is over Markov kernels.

  The Bayes risk of $(P, y, \ell')$ is $\mathcal R^*_B = \sup_{\pi \in \mathcal P(\Theta)} \mathcal R^P_\pi \: .$
\end{definition}

\emph{Example (SBHT):} $\mathcal R^*_B = \sup_{\gamma \in [0,1]}\inf_{\hat{y}}\left(\gamma \hat{\mu}_0(\hat{y} \ne 0) + (1 - \gamma) \hat{\mu}_1(\hat{y} \ne 1)\right)$.

\begin{definition}[Bayes estimator]
  \label{def:bayesEstimator}
  \lean{ProbabilityTheory.IsBayesEstimator}
  \leanok
  \uses{def:bayesRisk}
  An estimator $\hat{y}$ is said to be a Bayes estimator for a prior $\pi \in \mathcal P(\Theta)$ if $R^P_\pi(\hat{y}) = \mathcal R^P_\pi$.
\end{definition}

\begin{definition}[Minimax risk]
  \label{def:minimaxRisk}
  \lean{ProbabilityTheory.minimaxRisk}
  \leanok
  \uses{def:risk}
  The minimax risk of $(P, y, \ell')$ is $\mathcal R^* = \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} \sup_{\theta \in \Theta} r^P_\theta(\hat{y})$~.
\end{definition}

\emph{Example (SBHT):} $\mathcal R^* = \inf_{\hat{y}} \max\{\hat{\mu}_0(\hat{y} \ne 0), \hat{\mu}_1(\hat{y} \ne 1)\}$.

\begin{lemma}
  \label{lem:bayesRisk_le_minimaxRisk}
  \lean{ProbabilityTheory.bayesRisk_le_minimaxRisk}
  \leanok
  \uses{def:bayesRisk, def:minimaxRisk}
  $\mathcal R_B^* \le \mathcal R^*$.
\end{lemma}

\begin{proof}\leanok
For any $\pi \in \mathcal P(\mathcal X)$ and any estimator, $\pi\left[\hat{\mu}_\theta\left[\ell'(y(\theta), \hat{y}(\theta))\right]\right] \le \sup_{\theta \in \Theta}\hat{\mu}_\theta\left[\ell'(y(\theta), \hat{y}(\theta))\right]$.
\end{proof}

TODO: ``often'', $\mathcal R^*_B = \mathcal R^*$.

\subsection{Properties of the Bayes risk of a prior}

\begin{lemma}
  \label{lem:bayesRisk_le_const}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  The Bayes risk of a prior $\pi \in \mathcal M(\Theta)$ on $(P, y, \ell')$ satisfies
  \begin{align*}
  \mathcal R^P_\pi \le \inf_{z \in \mathcal Z} \pi\left[ \theta \mapsto \ell'(y(\theta), z) \right]
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{}
The infimum over all Markov kernels in the definition of the Bayes risk of $\pi$ is bounded from above by the infimum restricted to constant, deterministic Markov kernels.
\end{proof}

\begin{lemma}
  \label{lem:bayesianRisk_bayesInv}
  %\lean{}
  %\leanok
  \uses{def:bayesianRisk, def:bayesInv}
  The Bayesian risk of a Markov kernel $\hat{y} : \mathcal X \rightsquigarrow \mathcal Z$ with respect to a prior $\pi \in \mathcal M(\Theta)$ on $(P, y, \ell')$ satisfies
  \begin{align*}
  R^P_\pi(\hat{y}) = ((P_\pi^\dagger \times \hat{y}) \circ P \circ \pi)\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right] \: .
  \end{align*}
  In that expression, $P_\pi^\dagger$ is the Bayesian inverse of $P$ with respect to $\pi$.
\end{lemma}

\begin{proof}%\leanok
\uses{}
Use the main property of the Bayesian inverse.
\end{proof}

\begin{lemma}
  \label{lem:bayesianRisk_ge_inf_bayesInv}
  %\lean{}
  %\leanok
  \uses{def:bayesianRisk, def:bayesInv}
  The Bayesian risk of a Markov kernel $\hat{y} : \mathcal X \rightsquigarrow \mathcal Z$ with respect to a prior $\pi \in \mathcal M(\Theta)$ on $(P, y, \ell')$ satisfies
  \begin{align*}
  R^P_\pi(\hat{y}) \ge (P \circ \pi)\left[x \mapsto \inf_{z \in \mathcal Z} P_\pi^\dagger(x) \left[\theta \mapsto \ell'(y(\theta), z)\right]\right] \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesianRisk_bayesInv}
Starting from the equality of Lemma~\ref{lem:bayesianRisk_bayesInv}, we get
\begin{align*}
R^P_\pi(\hat{y})
&= ((P_\pi^\dagger \times \hat{y}) \circ P \circ \pi)\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
\\
&= (P \circ \pi)\left[x \mapsto (P_\pi^\dagger(x) \times \hat{y}(x)) \left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]\right]
\\
&= (P \circ \pi)\left[x \mapsto \hat{y}(x)\left[z \mapsto P_\pi^\dagger(x) \left[\theta \mapsto \ell'(y(\theta), z)\right]\right]\right]
\\
&\ge (P \circ \pi)\left[x \mapsto \inf_{z \in \mathcal Z} P_\pi^\dagger(x) \left[\theta \mapsto \ell'(y(\theta), z)\right]\right]
\: .
\end{align*}

\end{proof}

\begin{definition}[Generalized Bayes estimator]
  \label{def:genBayesEstimator}
  %\lean{}
  %\leanok
  \uses{def:risk,def:bayesInv}
  The generalized Bayes estimator for prior $\pi \in \mathcal P(\Theta)$ on $(P, y, \ell')$ is the deterministic estimator $\mathcal X \to \mathcal Z$ given by
  \begin{align*}
  x \mapsto \arg\min_z P_\pi^\dagger(x)\left[\theta \mapsto \ell'(y(\theta), z)\right] \: .
  \end{align*}
  TODO: is this measurable? Does the argmin exist? (under which assumptions?)
\end{definition}

\begin{lemma}
  \label{lem:bayesianRisk_genBayesEstimator}
  %\lean{}
  %\leanok
  \uses{def:bayesianRisk, def:genBayesEstimator}
  The Bayesian risk of the generalized Bayes estimator $\hat{y}_B$ is
  \begin{align*}
  R_\pi(\hat{y}_B) = (P \circ \pi)\left[x \mapsto \inf_{z \in \mathcal Z} P_\pi^\dagger(x) \left[\theta \mapsto \ell'(y(\theta), z)\right]\right]
  \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesianRisk_bayesInv}
Start from the equality of Lemma~\ref{lem:bayesianRisk_bayesInv} and use the definition of the generalized Bayes estimator.
\end{proof}

\begin{theorem}
  \label{thm:isBayesEstimator_genBayesEstimator}
  %\lean{}
  %\leanok
  \uses{def:bayesEstimator, def:genBayesEstimator}
  When the generalized Bayes estimator is well defined, it is a Bayes estimator.
\end{theorem}

\begin{proof}%\leanok
\uses{lem:bayesianRisk_ge_inf_bayesInv, lem:bayesianRisk_genBayesEstimator}
By Lemma~\ref{lem:bayesianRisk_ge_inf_bayesInv}, the Bayesian risk of the generalized Bayes estimator obtained in Lemma~\ref{lem:bayesianRisk_genBayesEstimator} is a lower bound for the Bayesian risk.
\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_bayesRisk}
  \lean{ProbabilityTheory.bayesRiskPrior_le_bayesRiskPrior_comp}
  \leanok
  \uses{def:bayesRisk}
  For $P : \Theta \rightsquigarrow \mathcal X$ and $\kappa : \mathcal X \rightsquigarrow \mathcal X'$ a Markov kernel, $\mathcal R^{\kappa \circ P}_\pi \ge \mathcal R^{P}_\pi$ (where the estimation problems differ only in the kernel).
\end{theorem}

\begin{proof}\leanok
\uses{}
The risk $\mathcal R^{\kappa \circ P}_\pi$ is
\begin{align*}
\inf_{\hat{y} : \mathcal X' \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ \kappa \circ P))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
&\ge \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ P))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
\: .
\end{align*}
The inequality is due to taking an infimum over a larger set: $\{\hat{y} \circ \kappa \mid \hat{y} : \mathcal X' \rightsquigarrow \{0,1\} \text{ Markov}\} \subseteq \{\hat{y}' : \mathcal X \rightsquigarrow \{0,1\} \mid \hat{y} \text{ Markov}\}$~.
\end{proof}

\begin{lemma}
  \label{lem:bayesRisk_compProd_le_fst}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  For $P : \Theta \rightsquigarrow \mathcal X$ and $\kappa : \Theta \times \mathcal X \rightsquigarrow \mathcal X'$ a Markov kernel, $\mathcal R^{P \otimes \kappa}_\pi \le \mathcal R^{P}_\pi$~.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:data_proc_bayesRisk}
Use Theorem~\ref{thm:data_proc_bayesRisk}: $P$ is the composition of $P \otimes \kappa$ and the deterministic kernel $\mathcal X \times \mathcal X' \rightsquigarrow \mathcal X$ that forgets the second value.
\end{proof}

\begin{lemma}
  \label{lem:bayesRisk_compProd_le_snd}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  For $P : \Theta \rightsquigarrow \mathcal X$ and $\kappa : \Theta \times \mathcal X \rightsquigarrow \mathcal X'$ a Markov kernel, $\mathcal R^{P \otimes \kappa}_\pi \le \mathcal R^{(P \otimes \kappa)_{\mathcal X'}}_\pi$~, in which $(P \otimes \kappa)_{\mathcal X'} : \mathcal \Theta \rightsquigarrow \mathcal X'$ is the kernel obtained by marginalizing over $\mathcal X$ in the output of $P \otimes \kappa$~.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:data_proc_bayesRisk}
Use Theorem~\ref{thm:data_proc_bayesRisk}: $(P \otimes \kappa)_{\mathcal X'}$ is the composition of $P \otimes \kappa$ and the deterministic kernel $\mathcal X \times \mathcal X' \rightsquigarrow \mathcal X'$ that forgets the first value.
\end{proof}

\begin{lemma}
  \label{lem:bayesRisk_concave}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  The Bayes risk $\mathcal R_\pi^P$ is concave in $P : \Theta \rightsquigarrow \mathcal X$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
The infimum of a sum is larger than the sum of the infimums:
\begin{align*}
\mathcal R_\pi^{\lambda P_1 + (1 - \lambda)P_2}
&= \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ (\lambda P_1 + (1 - \lambda)P_2)))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
\\
&= \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z}  \left( \lambda (\pi \otimes (\hat{y} \circ P_1))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right] \right.
  \\&\qquad \left. + (1 - \lambda) (\pi \otimes (\hat{y} \circ P_2))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right] \right)
\\
&\ge \lambda \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ P_1))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
  \\&\qquad + (1 - \lambda) \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ P_2))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
\\
&= \lambda \mathcal R_\pi^{P_1} + (1 - \lambda)\mathcal R_\pi^{P_2}
\: .
\end{align*}

\end{proof}




\subsection{Bayes risk increase}

The following is a new definition, which is a natural generalization of the DeGroot statistical information.

\begin{definition}
  \label{def:riskIncrease}
  %\lean{}
  %\leanok
  \uses{def:bayesRisk}
  The Bayes risk increase $I^P_{\pi}(\kappa)$ of kernel $\kappa : \mathcal X \rightsquigarrow \mathcal X'$ with respect to the estimation problem $(P, y, \ell')$ and the prior $\pi \in \mathcal M(\Theta)$ is the difference of the risk $\mathcal R_\pi$ of $(\kappa \circ P, y, \ell')$ and that of $(P, y, \ell')$. That is,
  \begin{align*}
  I^P_{\pi}(\kappa)
  &= \mathcal R^{\kappa \circ P}_\pi - \mathcal R^P_\pi
  \\
  &= \inf_{\hat{y} : \mathcal X' \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ \kappa \circ P))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
    - \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} (\pi \otimes (\hat{y} \circ P))\left[(\theta, z) \mapsto \ell'(y(\theta), z)\right]
  \: .
  \end{align*}
\end{definition}

In particular, if we take $\kappa : \mathcal X \rightsquigarrow *$ equal to the Markov kernel to the point space, we recover a difference between the risk of the best constant kernel and the Bayes estimator, as in the DeGroot statistical information.

The risk increase quantifies how much risk we accrue by forgetting information due to the composition with $\kappa$.

\begin{lemma}
  \label{lem:riskIncrease_nonneg}
  %\lean{}
  %\leanok
  \uses{def:riskIncrease}
  For $\kappa$ a Markov kernel, $I^P_\pi(\kappa) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:data_proc_bayesRisk}
Use Theorem~\ref{thm:data_proc_bayesRisk}.
\end{proof}

\begin{lemma}
  \label{lem:riskIncrease_comp}
  %\lean{}
  %\leanok
  \uses{def:riskIncrease}
  For $\kappa : \mathcal X \rightsquigarrow \mathcal X'$ and $\eta : \mathcal X' \rightsquigarrow \mathcal X''$~,
  $I^P_\pi(\eta \circ \kappa) = I^P_\pi(\kappa) + I^{\kappa \circ P}_\pi(\eta)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
\begin{align*}
I^P_\pi(\kappa) + I^{\kappa \circ P}_\pi(\eta)
&= \mathcal R^{\kappa \circ P}_\pi - \mathcal R^{P}_\pi + \mathcal R^{\eta \circ \kappa \circ P}_\pi - \mathcal R^{\kappa \circ P}_\pi
\\
&= \mathcal R^{\eta \circ \kappa \circ P}_\pi - \mathcal R^{P}_\pi
\\
&= I^P_\pi(\eta \circ \kappa)
\: .
\end{align*}
\end{proof}

\begin{lemma}[Data-processing inequality]
  \label{lem:riskIncrease_comp_del}
  %\lean{}
  %\leanok
  \uses{def:riskIncrease}
  For any measurable space $\mathcal X$, let $d_{\mathcal X} : \mathcal X \rightsquigarrow *$ be the Markov kernel to the point space.
  For all Markov kernels $\kappa : \mathcal X \rightsquigarrow \mathcal X'$,
  \begin{align*}
  I_\pi^P(d_{\mathcal X}) \ge I_\pi^{\kappa \circ P}(d_{\mathcal X'}) \: .
  \end{align*}
\end{lemma}

\begin{proof}%\leanok
\uses{lem:riskIncrease_comp, lem:riskIncrease_nonneg}
By Lemma~\ref{lem:riskIncrease_comp}, then Lemma~\ref{lem:riskIncrease_nonneg},
\begin{align*}
I_\pi^P(d_{\mathcal X'} \circ \kappa)
= I^P_\pi(\kappa) + I^{\kappa \circ P}_\pi(d_{\mathcal X'})
\ge I_\pi^{\kappa \circ P}(d_{\mathcal X'})
\: .
\end{align*}
Finally, $d_{\mathcal X'} \circ \kappa = d_{\mathcal X}$.
\end{proof}

\section{Simple binary hypothesis testing}

TODO: rework this section to use the Bayes risk increase.

In this section, for a measure $\xi$ on $\{0,1\}$, we write $\xi_0$ for $\xi(\{0\})$ and $\xi_1$ for $\xi(\{1\})$~. We sometimes write $(a,b)$ for the measure $\xi$ on $\{0,1\}$ with $\xi_0 = a$ and $\xi_1 = b$.

\subsection{Bayes risk of a prior}

\begin{definition}
  \label{def:bayesBinaryRisk}
  \lean{ProbabilityTheory.bayesBinaryRisk}
  \leanok
  \uses{def:bayesRisk}
  The Bayes binary risk between measures $\mu$ and $\nu$ with respect to prior $\xi \in \mathcal M(\{0,1\})$, denoted by $\mathcal B_\xi(\mu, \nu)$, is the Bayes risk $\mathcal R^P_\xi$ for $\Theta = \mathcal Y = \mathcal Z = \{0,1\}$, $\ell(y,z) = \mathbb{I}\{y \ne z\}$, $P$ the kernel sending 0 to $\mu$ and 1 to $\nu$ and prior $\xi$.
  That is,
  \begin{align*}
  \mathcal B_\xi(\mu, \nu) = \inf_{\hat{y} : \mathcal X \rightsquigarrow \{0,1\}}\left(\xi_0 (\hat{y} \circ \mu)(\{1\}) + \xi_1 (\hat{y} \circ \nu)(\{0\})\right)
  \: ,
  \end{align*}
  in which the infimum is over Markov kernels.

  If the prior is a probability measure with weights $(\pi, 1 - \pi)$, we write $B_\pi(\mu, \nu) = \mathcal B_{(\pi, 1 - \pi)}(\mu, \nu)$~.
\end{definition}

\begin{lemma}
  \label{lem:bayesBinaryRisk_le}
  \lean{ProbabilityTheory.bayesBinaryRisk_le_min}
  \leanok
  \uses{def:bayesBinaryRisk}
  For all measures $\mu, \nu$, $\mathcal B_\xi(\mu, \nu) \le \min\{\xi_0, \xi_1\}$ and $B_\pi(\mu, \nu) \le \min\{\pi, 1 - \pi\}$~.
\end{lemma}

\begin{proof}%\leanok
\uses{def:bayesRisk}
Since $\mathcal B_\xi(\mu, \nu)$ is an infimum over estimators, we can find an upper bound by choosing an estimator.
We choose for $\hat{y}$ a constant estimator, with value 1 if $\xi_1 > \xi_0$ and 0 otherwise.
That estimator has Bayesian risk $\min\{\xi_0, \xi_1\}$.
\end{proof}

\begin{lemma}
  \label{lem:bayesBinaryRisk_self}
  \lean{ProbabilityTheory.bayesBinaryRisk_self}
  \leanok
  \uses{def:bayesBinaryRisk}
  For $\mu \in \mathcal M(\mathcal X)$, $\mathcal B_\xi(\mu, \mu) = \min\{\xi_0, \xi_1\}$ and $B_\pi(\mu, \mu) = \min\{\pi, 1-\pi\}$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
\end{proof}

\begin{lemma}
  \label{lem:bayesBinaryRisk_symm}
  \lean{ProbabilityTheory.bayesBinaryRisk_symm}
  \leanok
  \uses{def:bayesBinaryRisk}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\xi \in \mathcal M(\{0,1\})$, $\mathcal B_\xi(\mu, \nu) = \mathcal B_{\xi_{\leftrightarrow}}(\nu, \mu)$ where $\xi_{\leftrightarrow} \in \mathcal M(\{0,1\})$ is such that $\xi_{\leftrightarrow}(\{0\}) = \xi_1$ and $\xi_{\leftrightarrow}(\{1\}) = \xi_0$.
  For $\pi \in [0,1]$, $B_\pi(\mu, \nu) = B_{1 - \pi}(\nu, \mu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_bayesBinaryRisk}
  \lean{ProbabilityTheory.bayesBinaryRisk_le_bayesBinaryRisk_comp}
  \leanok
  \uses{def:bayesBinaryRisk}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $\mathcal B_\xi(\kappa \circ \mu, \kappa \circ \nu) \ge \mathcal B_\xi(\mu, \nu)$~.
\end{theorem}

\begin{proof}\leanok
\uses{thm:data_proc_bayesRisk}
Apply Theorem~\ref{thm:data_proc_bayesRisk}.
\end{proof}

\begin{lemma}
  \label{lem:bayesInv_bayesBinaryRisk}
  %\lean{}
  %\leanok
  \uses{def:bayesInv, def:bayesBinaryRisk}
  The Bayesian inverse of the kernel $P$ in simple binary hypothesis testing with prior $\xi \in \mathcal M(\{0,1\})$ is $P_\xi^\dagger(x) = \left(\xi_0\frac{d \mu}{d(P \circ \xi)}(x), \xi_1\frac{d \nu}{d(P \circ \xi)}(x)\right)$ (almost surely w.r.t. $P \circ \xi = \xi_0 \mu + \xi_1 \nu$).
\end{lemma}

\begin{proof}%\leanok
\uses{lem:eq_bayesInv_of_compProd_eq}

\end{proof}

\begin{lemma}
  \label{lem:genBayesEstimator_bayesBinaryRisk}
  %\lean{}
  %\leanok
  \uses{def:genBayesEstimator, def:bayesBinaryRisk}
  The generalized Bayes estimator for the Bayes binary risk with prior $\xi \in \mathcal M(\{0,1\})$ is $x \mapsto \text{if } \xi_1\frac{d \nu}{d(P \circ \xi)}(x) \le \xi_0\frac{d \mu}{d(P \circ \xi)}(x) \text{ then } 0 \text{ else } 1$.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesInv_bayesBinaryRisk}
The generalized Bayes estimator is defined by
\begin{align*}
x \mapsto \arg\min_z P_\xi^\dagger(x)\left[\theta \mapsto \ell'(y(\theta), z)\right] \: .
\end{align*}
For the simple binary hypothesis testing problem,
\begin{align*}
P_\xi^\dagger(x)\left[\theta \mapsto \ell'(y(\theta), z)\right]
&= P_\xi^\dagger(x)(\theta \ne z)
\: .
\end{align*}
For $z = 0$, this is $P_\xi^\dagger(x)(\{1\})$ and for $z = 1$, this is $P_\xi^\dagger(x)(\{0\})$.
The generalized Bayes estimator is $x \mapsto \text{if } P_\xi^\dagger(x)(\{1\}) \le P_\xi^\dagger(x)(\{0\}) \text{ then } 0 \text{ else } 1$.

The expression of the lemma statement is obtained by replacing $P_\xi^\dagger$ by its value (Lemma~\ref{lem:bayesInv_bayesBinaryRisk}).
\end{proof}

\begin{theorem}
  \label{thm:bayesBinaryRisk_eq}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk}
  The Bayes risk of simple binary hypothesis testing for prior $\xi \in \mathcal M(\{0,1\})$ is
  \begin{align*}
  \mathcal B_\xi(\mu, \nu) = (P \circ \xi)\left[x \mapsto \min \left\{\xi_0\frac{d \mu}{d(P \circ \xi)}(x), \xi_1\frac{d \nu}{d(P \circ \xi)}(x)\right\}\right]
  \: .
  \end{align*}
\end{theorem}

\begin{proof}%\leanok
\uses{lem:bayesianRisk_genBayesEstimator, thm:isBayesEstimator_genBayesEstimator, lem:bayesInv_bayesBinaryRisk}
By Theorem~\ref{thm:isBayesEstimator_genBayesEstimator}, the generalized Bayes estimator is a Bayes estimator, hence it suffices to compute its Bayesian risk.
By Lemma~\ref{lem:bayesianRisk_genBayesEstimator},
\begin{align*}
R_\xi(\hat{y}_B)
&= (P \circ \xi)\left[x \mapsto \inf_{z \in \mathcal Z} P_\xi^\dagger(x) \left[\theta \mapsto \ell'(y(\theta), z)\right]\right]
\\
&= (P \circ \xi)\left[x \mapsto \min \left\{P_\xi^\dagger(x)(\{0\}), P_\xi^\dagger(x)(\{1\})\right\}\right]
\\
&= (P \circ \xi)\left[x \mapsto \min \left\{\xi_0\frac{d \mu}{d(P \circ \xi)}(x), \xi_1\frac{d \nu}{d(P \circ \xi)}(x)\right\}\right]
\: .
\end{align*}
The last line is obtained by replacing $P_\xi^\dagger$ by its value (Lemma~\ref{lem:bayesInv_bayesBinaryRisk}).
\end{proof}

\subsection{Statistical divergences between measures}

\begin{definition}
  \label{def:statInfo}
  %\lean{}
  %\leanok
  \uses{def:bayesBinaryRisk, def:riskIncrease}
  The statistical information between measures $\mu$ and $\nu$ with respect to prior $\xi \in \mathcal M(\{0,1\})$ is $\mathcal I_\xi(\mu, \nu) = \min\{\xi_0, \xi_1\} - \mathcal B_\xi(\mu, \nu)$.
  This is the risk increase $I_\xi^P(d_{\mathcal X})$ in the binary hypothesis testing problem for $d_{\mathcal X} : \mathcal X \rightsquigarrow *$ the Markov kernel to the point space.
\end{definition}

The statistical information is the difference between the minimal risk of an estimator that does not depend on the data and that of a Bayes estimator.
This is a simple generalization of both the DeGroot statistical information as well as the hockey-stick (or $E_\gamma$) divergence.

\begin{lemma}
  \label{lem:statInfo_self}
  %\lean{}
  %\leanok
  \uses{def:statInfo}
  For $\mu \in \mathcal M(\mathcal X)$, $\mathcal I_\xi(\mu, \mu) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_self}
Use Lemma~\ref{lem:bayesBinaryRisk_self}.
\end{proof}

\begin{lemma}
  \label{lem:statInfo_nonneg}
  %\lean{}
  %\leanok
  \uses{def:statInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$, $\mathcal I_\xi(\mu, \nu) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_le}
Use Lemma~\ref{lem:bayesBinaryRisk_le}.
\end{proof}

\begin{lemma}
  \label{lem:statInfo_symm}
  %\lean{}
  %\leanok
  \uses{def:statInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\xi \in \mathcal M(\{0,1\})$, $\mathcal I_\xi(\mu, \nu) = \mathcal I_{\xi_\leftrightarrow}(\nu, \mu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_symm}
Use Lemma~\ref{lem:bayesBinaryRisk_symm}.
\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_statInfo}
  %\lean{ProbabilityTheory.}
  %\leanok
  \uses{def:statInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$, $\xi \in \mathcal M(\{0,1\})$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $\mathcal I_\xi(\kappa \circ \mu, \kappa \circ \nu) \le \mathcal I_\xi(\mu, \nu)$~.
\end{theorem}

\begin{proof}%\leanok
\uses{thm:data_proc_bayesBinaryRisk, lem:riskIncrease_comp_del}
It suffices to prove $\mathcal B_\xi(\kappa \circ \mu, \kappa \circ \nu) \ge \mathcal B_\xi(\mu, \nu)$ since $\mathcal I_\xi(\mu, \nu) = \min\{\xi_0, \xi_1\} - \mathcal B_\xi(\mu, \nu)$.
The inequality was proved in Theorem~\ref{thm:data_proc_bayesBinaryRisk}.

Alternatively, we can use Lemma~\ref{lem:riskIncrease_comp_del}.
\end{proof}


\begin{definition}
  \label{def:deGrootInfo}
  \lean{ProbabilityTheory.deGrootInfo}
  \leanok
  \uses{def:bayesBinaryRisk, def:statInfo}
  The DeGroot statistical information between measures $\mu$ and $\nu$ for $\pi \in [0,1]$ is $I_\pi(\mu, \nu) = \min\{\pi, 1 - \pi\} - B_\pi(\mu, \nu)$.
  That is, $I_\pi(\mu, \nu) = \mathcal I_\xi(\mu, \nu)$ for a prior $\xi$ with $\xi_0 = \pi$ and $\xi_1 = 1-\pi$~.
\end{definition}

\begin{lemma}
  \label{lem:deGrootInfo_self}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu \in \mathcal M(\mathcal X)$, $I_\pi(\mu, \mu) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:statInfo_self}
Use Lemma~\ref{lem:statInfo_self}.
\end{proof}

\begin{lemma}
  \label{lem:deGrootInfo_nonneg}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$, $I_\pi(\mu, \nu) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:statInfo_nonneg}
Use Lemma~\ref{lem:statInfo_nonneg}.
\end{proof}

\begin{lemma}
  \label{lem:deGrootInfo_symm}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\pi \in [0,1]$, $I_\pi(\mu, \nu) = I_{1 - \pi}(\nu, \mu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:statInfo_symm}
Use Lemma~\ref{lem:statInfo_symm}.
\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_deGrootInfo}
  \lean{ProbabilityTheory.deGrootInfo_comp_le}
  \leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $I_\pi(\kappa \circ \mu, \kappa \circ \nu) \le I_\pi(\mu, \nu)$~.
\end{theorem}

\begin{proof}\leanok
\uses{thm:data_proc_statInfo}
Apply Theorem~\ref{thm:data_proc_statInfo}.
\end{proof}

TODO: once we prove the integral form of any $f$-divergence with respect to the DeGroot statistical information, this will extend the data-processing inequality for all $f$-divergences. 

\section{Reduction to testing}

TODO: lower bounds on estimation by reduction to testing problems.