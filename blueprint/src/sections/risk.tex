\chapter{Testing, Estimation, Risk}

In the estimation problem, we are given a family of measures and we want to estimate an unknown parameter for the measure from which we observe samples.
Let $\mathcal X$ be the measurable space on which the measures are defined.
The measure family is described by a kernel $P : \Theta \rightsquigarrow \mathcal X$. We write $P_\theta$ for $P(\theta)$.
In \emph{parametric} problems, $\Theta$ is a low-dimensional space. In \emph{nonparametric} problems $\Theta$ is large, for example $\Theta = \mathcal M (\mathcal X)$ or $\Theta = \mathcal P(\mathcal X)$.

The goal of the estimation task is to find from the observation of samples of $P_\theta$ for an unknown $\theta$ the value of a function $y : \Theta \to \mathcal Y$~, which maps the parameter $\theta$ to the quantity we are interested in. For example, $y(\theta)$ might be the expected value of the distribution $P_\theta$.

An estimator is a Markov kernel $\hat{y} : \mathcal X \rightsquigarrow \mathcal Z$, where most often $\mathcal Z = \mathcal Y$.
Note that while $y$ is a function of a parameter in $\Theta$, $\hat{y}$ has domain $\mathcal X$ as the estimation is done based on a sample of the measure, while the measure itself and its parameter remain unknown.
In general this is a randomized estimator, but often deterministic estimators (corresponding to deterministic kernels) are enough to attain optimal performance.
The quality of the estimation is quantified by a loss $\ell' : \mathcal Y \times \mathcal Z \to \mathbb{R}_{+, \infty}$. We write $\ell$ for a loss on $\mathcal Y \times \mathcal Y$ and $\ell'$ in the heterogeneous case.

\paragraph{Example: simple binary hypothesis testing (SBHT)}

We say that an estimation problem is a testing problem if $\mathcal Y = \mathcal Z$ is discrete and the loss takes values in $\{0, 1\}$.
A test is said to be binary if $\mathcal Y$ has two elements.
A test is simple if $\{\theta \mid y(\theta) = y_0\}$ is a singleton for all $y_0 \in \mathcal Y$.

In summary, in simple binary hypothesis testing, $\Theta = \mathcal Y = \mathcal Z = \{0,1\}$, $y(0) = 0$, $y(1) = 1$ (that is, $y$ is the identity). For $z \in \{0,1\}$, $\ell(y, z) = \mathbb{I}\{y \ne z\}$.

\section{Risk}

Let $\hat{\mu}_\theta$ be the law of $\hat{y}(\theta)$. That is, $\hat{\mu} : \mathcal \Theta \rightsquigarrow \mathcal X$ is the kernel $\hat{y} \circ P$.

\begin{definition}[Risk]
  \label{def:risk}
  \lean{ProbabilityTheory.risk}
  \leanok
  \uses{def:kernel_comp}
  The risk of an estimator $\hat{y}$ at $\theta \in \Theta$ is $r_\theta(\hat{y}) = \hat{\mu}_\theta\left[z \mapsto \ell'(y(\theta), z)\right]$~.
\end{definition}

\emph{Example (SBHT):} $r_\theta(\hat{y}) = \hat{\mu}_\theta(\hat{y}(\theta) \ne y(\theta))$.


\begin{definition}[Bayesian risk]
  \label{def:bayesianRisk}
  \lean{ProbabilityTheory.bayesianRisk}
  \leanok
  \uses{def:risk}
  The Bayesian risk of an estimator $\hat{y}$ for a prior $\pi \in \mathcal P(\Theta)$ is $R_\pi(\hat{y}) = \pi\left[\theta \mapsto r_\theta(\hat{y})\right]$~.
\end{definition}


\begin{definition}[Bayes risk]
  \label{def:bayesRisk}
  \lean{ProbabilityTheory.bayesRisk}
  \leanok
  \uses{def:bayesianRisk}
  The Bayes risk for prior $\pi \in \mathcal P(\Theta)$ is $\mathcal R_\pi = \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} R_\pi(\hat{y})$~, where the infimum is over Markov kernels.

  The Bayes risk of $P$ is $\mathcal R^*_B = \sup_{\pi \in \mathcal P(\Theta)} \mathcal R_\pi \: .$
\end{definition}

\emph{Example (SBHT):} $\mathcal R^*_B = \sup_{\gamma \in [0,1]}\inf_{\hat{y}}\left(\gamma \hat{\mu}_0(\hat{y} \ne 0) + (1 - \gamma) \hat{\mu}_1(\hat{y} \ne 1)\right)$.

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_bayesRisk}
  \lean{ProbabilityTheory.bayesRiskPrior_le_bayesRiskPrior_comp}
  \leanok
  \uses{def:bayesRisk}
  For $P : \Theta \rightsquigarrow \mathcal X$ and $\kappa : \mathcal X \rightsquigarrow \mathcal X'$ a Markov kernel, the risk $\mathcal R_\pi$ of $\kappa \circ P$ is larger than $\mathcal R_\pi$ for $P$.
\end{theorem}

\begin{proof}\leanok
\uses{}
The risk $\mathcal R_\pi$ of $\kappa \circ P$ is
\begin{align*}
\inf_{\hat{y} : \mathcal X' \rightsquigarrow \mathcal Z} \pi\left[\theta \mapsto (\hat{y} \circ \kappa \circ P)(\theta)\left[z \mapsto \ell'(y(\theta), z)\right]\right]
&\le \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} \pi\left[\theta \mapsto (\hat{y} \circ P)(\theta)\left[z \mapsto \ell'(y(\theta), z)\right]\right]
\: .
\end{align*}
The inequality is due to taking an infimum over a larger set: $\{\hat{y} \circ \kappa \mid \hat{y} : \mathcal X' \rightsquigarrow \{0,1\} \text{ Markov}\} \subseteq \{\hat{y}' : \mathcal X \rightsquigarrow \{0,1\} \mid \hat{y} \text{ Markov}\}$~.
\end{proof}


\begin{definition}[Bayes estimator]
  \label{def:bayesEstimator}
  \lean{ProbabilityTheory.IsBayesEstimator}
  \leanok
  \uses{def:bayesRisk}
  An estimator $\hat{y}$ is said to be a Bayes estimator for a prior $\pi \in \mathcal P(\Theta)$ if $R_\pi(\hat{y}) = \mathcal R_\pi$.
\end{definition}

\begin{definition}[Generalized Bayes estimator]
  \label{def:genBayesEstimator}
  %\lean{}
  %\leanok
  \uses{def:risk,def:bayesInv}
  The generalized Bayes estimator for prior $\pi \in \mathcal P(\Theta)$ and kernel $P: \Theta \rightsquigarrow \mathcal X$ is the deterministic estimator $\mathcal X \to \mathcal Z$ given by
  \begin{align*}
  x \mapsto \arg\min_z P_\pi^\dagger(x)\left[\theta \mapsto \ell'(y(\theta), z)\right] \: .
  \end{align*}
  Here $P_\pi^\dagger$ is the Bayesian inverse of $P$ with respect to $\pi$ (that is, $P_\pi^\dagger(x)$ is the posterior distribution of $\theta$ given $x$).
\end{definition}

\begin{definition}[Minimax risk]
  \label{def:minimaxRisk}
  \lean{ProbabilityTheory.minimaxRisk}
  \leanok
  \uses{def:risk}
  The minimax risk of $P$ is $\mathcal R^* = \inf_{\hat{y} : \mathcal X \rightsquigarrow \mathcal Z} \sup_{\theta \in \Theta} r_\theta(\hat{y})$~.
\end{definition}

\emph{Example (SBHT):} $\mathcal R^* = \inf_{\hat{y}} \max\{\hat{\mu}_0(\hat{y} \ne 0), \hat{\mu}_1(\hat{y} \ne 1)\}$.

\begin{lemma}
  \label{lem:bayesRisk_le_minimaxRisk}
  \lean{ProbabilityTheory.bayesRisk_le_minimaxRisk}
  \leanok
  \uses{def:bayesRisk, def:minimaxRisk}
  $\mathcal R_B^* \le \mathcal R^*$.
\end{lemma}

\begin{proof}\leanok
For any $\pi \in \mathcal P(\mathcal X)$ and any estimator, $\pi\left[\hat{\mu}_\theta\left[\ell'(y(\theta), \hat{y}(\theta))\right]\right] \le \sup_{\theta \in \Theta}\hat{\mu}_\theta\left[\ell'(y(\theta), \hat{y}(\theta))\right]$.
\end{proof}

TODO: ``often'', $\mathcal R^*_B = \mathcal R^*$.

\section{Simple binary hypothesis testing}

\begin{definition}
  \label{def:bayesBinaryRisk}
  \lean{ProbabilityTheory.bayesBinaryRisk}
  \leanok
  \uses{def:bayesRisk}
  The Bayes binary risk between measures $\mu$ and $\nu$, denoted by $B_\pi(\mu, \nu)$ and parametrized by $\pi \in [0,1]$, is the Bayes risk $\mathcal R_\pi$ for $\Theta = \mathcal Y = \mathcal Z = \{0,1\}$, $\ell(y,z) = \mathbb{I}\{y \ne z\}$, $P$ the kernel sending 0 to $\mu$ and 1 to $\nu$ and prior $(\pi, 1-\pi)$.
  That is,
  \begin{align*}
  B_\pi(\mu, \nu) = \inf_{\hat{y} : \mathcal X \rightsquigarrow \{0,1\}}\left(\pi (\hat{y} \circ \mu)(\{1\}) + (1 - \pi) (\hat{y} \circ \nu)(\{0\})\right)
  \: ,
  \end{align*}
  in which the infimum is over Markov kernels.
\end{definition}

\begin{lemma}
  \label{lem:bayesBinaryRisk_le}
  \lean{ProbabilityTheory.bayesBinaryRisk_le_min}
  \leanok
  \uses{def:bayesBinaryRisk}
  For all probability measures $\mu, \nu$, $B_\pi(\mu, \nu) \le \min\{\pi, 1 - \pi\}$~.
\end{lemma}

\begin{proof}%\leanok
\uses{def:bayesRisk}
Since $B_\pi(\mu, \nu)$ is an infimum over estimators, we can find an upper bound by choosing an estimator. We choose for $\hat{y}$ a constant estimator, with value 1 if $1- \pi > \pi$ and 0 otherwise.
That estimator has Bayesian risk $\min\{\pi, 1 - \pi\}$.
\end{proof}

\begin{lemma}
  \label{lem:bayesBinaryRisk_self}
  \lean{ProbabilityTheory.bayesBinaryRisk_self}
  \leanok
  \uses{def:bayesBinaryRisk}
  For $\mu \in \mathcal P(\mathcal X)$, $B_\pi(\mu, \mu) = \min\{\pi, 1-\pi\}$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
\end{proof}

\begin{lemma}
  \label{lem:bayesBinaryRisk_symm}
  \lean{ProbabilityTheory.bayesBinaryRisk_symm}
  \leanok
  \uses{def:bayesBinaryRisk}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\pi \in [0,1]$, $B_\pi(\mu, \nu) = B_{1 - \pi}(\nu, \mu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}

\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_bayesBinaryRisk}
  \lean{ProbabilityTheory.bayesBinaryRisk_le_bayesBinaryRisk_comp}
  \leanok
  \uses{def:bayesBinaryRisk}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $B_\pi(\kappa \circ \mu, \kappa \circ \nu) \ge B_\pi(\mu, \nu)$~.
\end{theorem}

\begin{proof}\leanok
\uses{thm:data_proc_bayesRisk}
Apply Theorem~\ref{thm:data_proc_bayesRisk}.
\end{proof}

\begin{definition}
  \label{def:deGrootInfo}
  \lean{ProbabilityTheory.deGrootInfo}
  \leanok
  \uses{def:bayesBinaryRisk}
  The DeGroot statistical information between measures $\mu$ and $\nu$ is $I_\pi(\mu, \nu) = \min\{\pi, 1 - \pi\} - B_\pi(\mu, \nu)$.
\end{definition}

\begin{lemma}
  \label{lem:deGrootInfo_self}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu \in \mathcal P(\mathcal X)$, $I_\pi(\mu, \mu) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_self}
Use Lemma~\ref{lem:bayesBinaryRisk_self}.
\end{proof}

\begin{lemma}
  \label{lem:deGrootInfo_nonneg}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal P(\mathcal X)$, $I_\pi(\mu, \nu) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_le}
Use Lemma~\ref{lem:bayesBinaryRisk_le}.
\end{proof}

\begin{lemma}
  \label{lem:deGrootInfo_symm}
  %\lean{}
  %\leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\pi \in [0,1]$, $I_\pi(\mu, \nu) = I_{1 - \pi}(\nu, \mu)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:bayesBinaryRisk_symm}
Use Lemma~\ref{lem:bayesBinaryRisk_symm}.
\end{proof}

\begin{theorem}[Data-processing inequality]
  \label{thm:data_proc_deGrootInfo}
  \lean{ProbabilityTheory.deGrootInfo_comp_le}
  \leanok
  \uses{def:deGrootInfo}
  For $\mu, \nu \in \mathcal M(\mathcal X)$ and $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ a Markov kernel, $I_\pi(\kappa \circ \mu, \kappa \circ \nu) \le I_\pi(\mu, \nu)$~.
\end{theorem}

Note that this theorem is valid for any measurable space: no countability is required.

\begin{proof}\leanok
\uses{thm:data_proc_bayesBinaryRisk}
It suffices to prove $B_\pi(\kappa \circ \mu, \kappa \circ \nu) \ge B_\pi(\mu, \nu)$ since $I_\pi(\mu, \nu) = \min\{\pi, 1 - \pi\} - B_\pi(\mu, \nu)$.
The inequality was proved in Theorem~\ref{thm:data_proc_bayesBinaryRisk}.
\end{proof}

TODO: this extends the data-processing inequality for TV to any space.

TODO: once we prove the integral form of any $f$-divergence with respect to the DeGroot statistical information, this will extend the data-processing inequality for all $f$-divergences. 

\section{Deficiency}

TODO: this part is not used elsewhere for now, and should perhaps be deleted.
In any case, it should be placed somewhere after the definition of$\TV$.

\begin{definition}[Le Cam deficiency]
  \label{def:deficiency}
  %\lean{}
  %\leanok
  \uses{def:TV}
  The Le Cam deficiency of $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ with respect to $\eta : \mathcal X \rightsquigarrow \mathcal Z$ is
  \begin{align*}
  \delta_{\TV}(\kappa, \eta) = \inf_{\xi} \sup_{x \in \mathcal X} \TV(\xi \circ \kappa (x), \eta(x)) \: ,
  \end{align*}
  in which the infimum is taken over all Markov kernels $\xi : \mathcal Y \rightsquigarrow \mathcal Z$~.
\end{definition}

TODO: from any divergence on measures $D$, \cite{perrone2023markov} defines a divergence between kernels $\kappa, \kappa' : \mathcal X \rightsquigarrow \mathcal Y$ by $D(\kappa, \kappa') = \sup_{x \in \mathcal X} D(\kappa(x), \eta(x))$.
If we see measures as kernels from a point space, the divergence between measures-as-kernels coincides with the divergence between the measures.
With that notation, $\delta_{\TV}(\kappa, \eta) = \inf_{\xi} \TV(\xi \circ \kappa, \eta)$.
We could also proceed as \cite{raginsky2011shannon} and extend the notion of deficiency to deficiency with respect to a divergence: $\delta_D(\kappa, \eta) = \inf_{\xi} D(\eta, \xi\circ\kappa)$ (note the reversed arguments, which does not matter for the case of $\TV$ since it is symmetric).

\begin{lemma}
  \label{lem:deficiency_nonneg}
  %\lean{}
  %\leanok
  \uses{def:deficiency}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$ and $\eta : \mathcal X \rightsquigarrow \mathcal Z$. Then $\delta_{\TV}(\kappa, \eta) \ge 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:tv_nonneg}
This is a consequence of Lemma~\ref{lem:tv_nonneg}.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_self}
  %\lean{}
  %\leanok
  \uses{def:deficiency}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$. Then $\delta_{\TV}(\kappa, \kappa) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:deficiency_nonneg}
By Lemma~\ref{lem:deficiency_nonneg}, $0 \le \delta_{\TV}(\kappa, \kappa)$.
In the infimum that defines $\delta_{\TV}(\kappa, \kappa)$, take $\xi = \textup{id}$ to get an upper bound of 0 on $\delta_{\TV}(\kappa, \kappa)$.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_comp}
  %\lean{}
  %\leanok
  \uses{def:deficiency, def:blackwellOrder}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$ and $\eta : \mathcal X \rightsquigarrow \mathcal Z$.
  If $\eta \le_B \kappa$ (that is, there exists a Markov kernel $\chi : \mathcal Y \rightsquigarrow \mathcal Z$ such that $\eta = \chi \circ \kappa$), then $\delta_{\TV}(\kappa, \eta) = 0$~.
\end{lemma}

\begin{proof}%\leanok
\uses{lem:deficiency_nonneg}
In the infimum that defines $\delta_{\TV}(\kappa, \eta)$, take $\xi = \chi$ to get an upper bound of 0 on $\delta_{\TV}(\kappa, \eta)$. The lower bound is Lemma~\ref{lem:deficiency_nonneg}.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_antimono_left}
  %\lean{}
  %\leanok
  \uses{def:deficiency, def:blackwellOrder}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$, $\eta : \mathcal X \rightsquigarrow \mathcal Z$ and $\zeta : \mathcal X \rightsquigarrow \mathcal W$ be Markov kernels.
  If $\eta \le_B \kappa$ then $\delta_{\TV}(\kappa, \zeta) \le \delta_{\TV}(\eta, \zeta)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{}
Let $\chi$ be a Markov kernel such that $\eta = \chi \circ \kappa$~.
\begin{align*}
\delta_{\TV}(\eta, \zeta)
&= \inf_{\xi} \sup_{x \in \mathcal X} \TV(\xi \circ \eta (x), \zeta(x))
\\
&= \inf_\xi \sup_{x \in \mathcal X} \TV(\xi \circ \chi \circ \kappa (x), \zeta(x))
\: .
\end{align*}
Since $\xi \circ \chi$ is a Markov kernel, the infimum over Markov kernels of the form $\xi \circ \chi$ is larger than the infimum over all Markov kernels.
\end{proof}

\begin{lemma}
  \label{lem:deficiency_mono_right}
  %\lean{}
  %\leanok
  \uses{def:deficiency, def:blackwellOrder}
  Let $\kappa: \mathcal X \rightsquigarrow \mathcal Y$, $\eta : \mathcal X \rightsquigarrow \mathcal Z$ and $\zeta : \mathcal X \rightsquigarrow \mathcal W$ be Markov kernels.
  If $\zeta \le_B \eta$ then $\delta_{\TV}(\kappa, \zeta) \le \delta_{\TV}(\kappa, \eta)$~.
\end{lemma}

\begin{proof}%\leanok
\uses{thm:tv_data_proc}
Let $\chi$ be a Markov kernel such that $\zeta = \chi \circ \eta$~.
\begin{align*}
\delta_{\TV}(\kappa, \zeta)
&= \inf_{\xi : \mathcal Y \rightsquigarrow \mathcal W} \sup_{x \in \mathcal X} \TV(\xi \circ \kappa (x), \zeta(x))
\\
&= \inf_{\xi : \mathcal Y \rightsquigarrow \mathcal W} \sup_{x \in \mathcal X} \TV(\xi \circ \kappa (x), \chi \circ \eta(x))
\\
&\le \inf_{\xi' : \mathcal Y \rightsquigarrow \mathcal Z} \sup_{x \in \mathcal X} \TV(\chi \circ \xi' \circ \kappa (x), \chi \circ \eta(x))
\: .
\end{align*}
By the data-processing inequality for $\TV$ (Theorem \ref{thm:tv_data_proc}), for all $\xi' : \mathcal Y \rightsquigarrow \mathcal Z$, $\TV(\chi \circ \xi' \circ \kappa (x), \chi \circ \eta(x)) \le \TV(\xi' \circ \kappa (x), \eta(x))$.
This concludes the proof.
\end{proof}

\section{Reduction to testing}

TODO: lower bounds on estimation by reduction to testing problems.