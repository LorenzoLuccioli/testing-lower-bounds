\chapter{f-divergences}

\section{Definition and basic properties}

Everywhere in this document, the functions used in divergences are assumed measurable and convex on $[0, +\infty)$.
For such a function $f : \mathbb{R} \to \mathbb{R}$, we write $f'(\infty) = \lim_{x \to +\infty} f(x)/x$ (which can be infinite).

For $\mu, \nu$ two measures, we denote by $\mu_{\perp \nu}$ the singular part of $\mu$ with respect to $\nu$.

\begin{definition}[f-divergence]
  \label{def:fDiv}
  \lean{ProbabilityTheory.fDiv}
  \leanok
  \uses{def:derivAtTop}
  Let $f : \mathbb{R} \to \mathbb{R}$ and let $\mu, \nu$ be two measures on a measurable space $\mathcal X$. The f-divergence between $\mu$ and $\nu$ is
  \begin{align*}
  D_f(\mu, \nu) = \nu\left[x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)\right] + f'(\infty) \mu_{\perp \nu}(\mathcal X)
  \end{align*}
  if $x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)$ is $\nu$-integrable and $+\infty$ otherwise.
\end{definition}

\begin{lemma}
  \label{lem:fDiv_ne_top_iff}
  \lean{ProbabilityTheory.fDiv_ne_top_iff}
  \leanok
  \uses{def:fDiv}
  For $\mu$ and $\nu$ two finite measures, $D_f(\mu, \nu)$ is finite if and only if $x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)$ is $\nu$-integrable and either $f'(\infty) < \infty$ or $\mu \ll \nu$.
\end{lemma}

\begin{proof}\leanok
\end{proof}

\begin{lemma}
  \label{lem:fDiv_const}
  \lean{ProbabilityTheory.fDiv_const}
  \leanok
  \uses{def:fDiv}
  For $\nu$ a finite measure, for all $a \in \mathbb{R}$, $D_{x \mapsto a}(\mu, \nu) = a \nu(\mathcal X)$.
\end{lemma}

\begin{proof} \leanok
Compute the integral.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_self}
  \lean{ProbabilityTheory.fDiv_self}
  \leanok
  \uses{def:fDiv}
  If $f(1) = 0$ then $D_{f}(\mu, \mu) = 0$.
\end{lemma}

\begin{proof} \leanok
$\frac{d \mu}{d \mu}(x) = 1$ almost everywhere and $f(1) = 0$.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_id}
  \lean{ProbabilityTheory.fDiv_id}
  \leanok
  \uses{def:fDiv}
  $D_{x \mapsto x}(\mu, \nu) = \mu(\mathcal X)$.
\end{lemma}

\begin{proof} \leanok
Compute the integral: its value is $(\frac{d\mu}{d\nu}\cdot \nu)(\mathcal X)$. Then
$D_{x\mapsto x}(\mu, \nu) = (\frac{d\mu}{d\nu}\cdot \nu)(\mathcal X) + \mu_{\perp \nu}(\mathcal X) = \mu (\mathcal X)$.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_mul}
  \lean{ProbabilityTheory.fDiv_mul}
  \leanok
  \uses{def:fDiv}
  For all $a \ge 0$, $D_{a f}(\mu, \nu) = a D_{f}(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
Linearity of the integral.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_add}
  \lean{ProbabilityTheory.fDiv_add}
  \leanok
  \uses{def:fDiv}
  $D_{f + g}(\mu, \nu) = D_f(\mu, \nu) + D_g(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
Linearity of the integral.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_add_linear}
  \lean{ProbabilityTheory.fDiv_add_linear}
  \leanok
  \uses{def:fDiv}
  For finite measures $\mu$ and $\nu$ with $\mu(\mathcal X) = \nu(\mathcal X)$, for all $a \ge 0$, $D_{f + a(x - 1)}(\mu, \nu) = D_{f}(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_add, lem:fDiv_mul, lem:fDiv_const, lem:fDiv_id}
Linearity (Lemmas~\ref{lem:fDiv_add} and~\ref{lem:fDiv_mul}), then Lemma~\ref{lem:fDiv_const} and~\ref{lem:fDiv_id}.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_map_measurableEmbedding}
  \lean{ProbabilityTheory.fDiv_map_measurableEmbedding}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X$ and let $g : \mathcal X \to \mathcal Y$ be a measurable embedding. Then $D_f(g_* \mu, g_* \nu) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\end{proof}

\begin{lemma}
  \label{lem:fDiv_eq_add_withDensity_derivAtTop}
  \lean{ProbabilityTheory.fDiv_eq_add_withDensity_derivAtTop}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two finite measures on $\mathcal X$.
  Then $D_f(\mu, \nu) = D_f(\frac{d\mu}{d\nu}\cdot \nu, \nu) + f'(\infty) \mu_{\perp \nu}(\mathcal X)$.
\end{lemma}

\begin{proof}\leanok
\end{proof}

\begin{lemma}
  \label{lem:le_fDiv_of_ac}
  \lean{ProbabilityTheory.le_fDiv_of_ac}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ be a finite measure and $\nu$ be a probability measure on the same space $\mathcal X$, such that $\mu \ll \nu$. Then $f(\mu(\mathcal X)) \le D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
Since $\mu \ll \nu$ the $f$-divergence is only the integral part. Then by Jensen's inequality,
\begin{align*}
D_f(\mu, \nu)
&= \nu\left[ f\left( \frac{d\mu}{d\nu} \right) \right]
\ge f\left( \nu\left[\frac{d\mu}{d\nu} \right] \right)
= f(\mu(\mathcal X))
\: .
\end{align*}

\end{proof}

\begin{lemma}
  \label{lem:le_fDiv}
  \lean{ProbabilityTheory.le_fDiv}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ be a finite measure and $\nu$ be a probability measure on the same space $\mathcal X$. Then $f(\mu(\mathcal X)) \le D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:fDiv_eq_add_withDensity_derivAtTop, lem:le_fDiv_of_ac}
By convexity, then Lemma~\ref{lem:le_fDiv_of_ac} and finally Lemma~\ref{lem:fDiv_eq_add_withDensity_derivAtTop},
\begin{align*}
f(\mu(\mathcal X))
&\le f(\frac{d\mu}{d\nu}\cdot \nu (\mathcal X)) + f'(\infty)\mu_{\perp}(\mathcal X)
\\
&\le D_f(\frac{d\mu}{d\nu}\cdot \nu , \nu) + f'(\infty)\mu_{\perp}(\mathcal X)
\\
&= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}

\begin{lemma}
  \label{lem:fDiv_nonneg}
  \lean{ProbabilityTheory.fDiv_nonneg}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two probability measures. If $f(1) = 0$ then $D_f(\mu, \nu) \ge 0$.
\end{lemma}

\begin{proof}\leanok
\uses{lem:le_fDiv}
Apply Lemma~\ref{lem:le_fDiv} and use $f(\mu(\mathcal X)) = f(1) = 0$.
\end{proof}

\section{Data-processing inequality: sigma-algebras}

TODO

\section{Conditional f-divergence}

\begin{definition}[Conditional f-divergence]
  \label{def:condFDiv}
  \lean{ProbabilityTheory.condFDiv}
  \leanok
  \uses{def:fDiv}
  Let $f : \mathbb{R} \to \mathbb{R}$, $\mu$ a measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ two Markov kernels from $\mathcal X$ to $\mathcal Y$. The conditional f-divergence between $\kappa$ and $\eta$ with respect to $\mu$ is
  \begin{align*}
  D_f(\kappa, \eta \mid \mu) = \mu\left[x \mapsto D_f(\kappa(x), \eta(x))\right]
  \end{align*}
  if $x \mapsto D_f(\kappa(x), \eta(x))$ is $\mu$-integrable and $+\infty$ otherwise.
\end{definition}

\begin{lemma}
  \label{lem:integrable_fDiv_compProd_iff}
  \lean{ProbabilityTheory.integrable_f_rnDeriv_compProd_iff}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels from $\mathcal X$ to $\mathcal Y$.
  Then $p \mapsto f \left(\frac{d(\mu \otimes \kappa)}{d(\mu \otimes \eta)}(p)\right)$ is $(\mu \otimes \eta)$-integrable iff
  \begin{itemize}
    \item $x \mapsto D_f(\kappa(x), \eta(x))$ is $\mu$-integrable and
    \item for $\mu$-almost all $x$, $y \mapsto f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right)$ is $\eta(x)$-integrable. 
  \end{itemize}
\end{lemma}

\begin{proof}
Since $x \mapsto f \left(\frac{d(\mu \otimes \kappa)}{d(\mu \otimes \eta)} x \right)$ is measurable, the integrability condition w.r.t. $\mu \otimes \eta$ is equivalent to
\begin{itemize}
    \item $x \mapsto \int_y \left\Vert f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \right\Vert \partial \eta(x)$ is $\mu$-integrable and
    \item for $\mu$-almost all $x$, $y \mapsto f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right)$ is $\eta(x)$-integrable. 
  \end{itemize}
It suffices to use convexity to show that integrability of $x \mapsto \int_y f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \partial \eta(x)$ implies integrability of $x \mapsto \int_y \left\Vert f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \right\Vert \partial \eta(x)$.

TODO
\end{proof}

\begin{lemma}
  \label{lem:condFDiv_ne_top_iff}
  \lean{ProbabilityTheory.condFDiv_ne_top_iff}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels, where $\kappa$ is a Markov kernel.
  Then $D_f(\kappa, \eta \mid \mu) \ne \infty$ if and only if
  \begin{itemize}
    \item for $\mu$-almost all $x$, $y \mapsto f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right)$ is $\eta(x)$-integrable,
    \item $x \mapsto \int_y f \left( \frac{d\kappa(x)}{d\eta(x)}(y) \right) \partial \eta(x)$ is $\mu$-integrable,
    \item either $f'(\infty) < \infty$ or for $\mu$-almost all $x$, $\kappa(x) \ll \eta(x)$.
  \end{itemize}
\end{lemma}

\begin{proof} \leanok
\uses{lem:integrable_fDiv_compProd_iff}
\end{proof}

\begin{lemma}
  \label{lem:fDiv_compProd_ne_top_iff}
  \lean{ProbabilityTheory.condFDiv_ne_top_iff_fDiv_compProd_ne_top}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels, where $\kappa$ is a Markov kernel.
  Then $D_f(\mu \otimes \kappa, \mu \otimes \eta) \ne \infty \iff D_f(\kappa, \eta \mid \mu) \ne \infty$.
\end{lemma}

\begin{proof} \leanok
\uses{lem:integrable_fDiv_compProd_iff, lem:condFDiv_ne_top_iff}
\end{proof}

\begin{lemma}
  \label{lem:fDiv_compProd_left}
  \lean{ProbabilityTheory.fDiv_compProd_left}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a finite measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two finite kernels.
  Then $D_f(\mu \otimes \kappa, \mu \otimes \eta) = D_f(\kappa, \eta \mid \mu)$.
\end{lemma}

\begin{proof} \leanok
\uses{cor:rnDeriv_compProd_right, cor:rnDeriv_value, lem:fDiv_compProd_ne_top_iff, lem:condFDiv_ne_top_iff}
By Lemma~\ref{lem:fDiv_compProd_ne_top_iff}, the conditions on which the two divergences are finite are the same. We then assume integrability properties such that both are finite (given by Lemma~\ref{lem:condFDiv_ne_top_iff}).

TODO: the following proof assumes $\kappa(x) \ll \eta(x)$ for $\nu$-almost all $x$. Describe the general case.

By Lemma~\ref{cor:rnDeriv_compProd_right} and Corollary~\ref{cor:rnDeriv_value},
\begin{align*}
D_f(\mu \otimes \kappa, \mu \otimes \eta)
&= \int_{p} f\left(\frac{d (\mu \otimes \kappa)}{d (\mu \otimes \eta)}(p)\right) \partial(\mu \otimes \eta)
\\
&= \int_{p} f\left(\frac{d \kappa}{d \eta}(p)\right) \partial(\mu \otimes \eta)
\\
&= \int_x \int_y f\left(\frac{d \kappa}{d \eta}(x, y)\right) \partial \eta(x) \partial \mu
\\
&= \int_x \int_y f\left(\frac{d \kappa(x)}{d \eta(x)}(y)\right) \partial \eta(x) \partial \mu
\\
&= \mu\left[D_f(\kappa(x), \eta(x))\right]
= D_f(\kappa, \eta \mid \mu)
\: .
\end{align*}
\end{proof}

\section{Data-processing inequality}

\begin{theorem}
  \label{thm:fDiv_le_compProd}
  \lean{ProbabilityTheory.le_fDiv_compProd}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $D_f(\mu, \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$.
\end{theorem}

\begin{proof}\leanok
\uses{lem:rnDeriv_compProd, cor:rnDeriv_value}
TODO: the following proof assumes $\mu \ll \nu$ and $\kappa(x) \ll \eta(x)$ for $\nu$-almost all $x$. Describe the general case.

Using Lemma~\ref{lem:rnDeriv_compProd} and \ref{cor:rnDeriv_value},
\begin{align*}
D(\mu \otimes \kappa, \nu \otimes \eta)
&= \int_x \int_y f \left( \frac{\partial \mu}{\partial\nu}(x) \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \right) \partial \eta(x) \partial \nu
\: .
\end{align*}
Since $f$ is convex, by Jensen's inequality,
\begin{align*}
\int_y f \left( \frac{\partial \mu}{\partial\nu}(x) \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \right) \partial \eta(x)
&\ge f \left( \int_y \frac{\partial \mu}{\partial\nu}(x) \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \partial \eta(x) \right)
\\
&= f \left( \frac{\partial \mu}{\partial\nu}(x) \int_y \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \partial \eta(x) \right)
\: .
\end{align*}

Since $\kappa(x) \ll \eta(x)$ for $\nu$-almost all $x$, $\int_y \frac{\partial \kappa(x)}{\partial\eta(x)}(y) \partial \eta(x) = \int_y 1 \partial \kappa(x) = 1$ a.e.. We have obtained
\begin{align*}
D_f(\mu \otimes \kappa, \nu \otimes \eta)
\ge \int_x f \left( \frac{\partial \mu}{\partial\nu}(x)\right) \partial \nu
= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}

\begin{theorem}[Marginals]
  \label{thm:fDiv_fst_le}
  \lean{ProbabilityTheory.fDiv_fst_le}
  \leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X \times \mathcal Y$ where $\mathcal Y$ is standard Borel, and let $\mu_X, \nu_X$ be their marginals on $\mathcal X$.
  Then $D_f(\mu_X, \nu_X) \le D_f(\mu, \nu)$.
  Similarly, for $\mathcal X$ standard Borel and $\mu_Y, \nu_Y$ the marginals on $\mathcal Y$, $D_f(\mu_Y, \nu_Y) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{thm:fDiv_le_compProd, lem:fDiv_map_measurableEmbedding}
We introduce conditional kernels and write $D(\mu, \nu) = D(\mu_X \otimes \mu_{Y|X}, \nu_X \otimes \nu_{Y|X})$. Then apply Theorem~\ref{thm:fDiv_le_compProd}.
For the second statement, we need Lemma~\ref{lem:fDiv_map_measurableEmbedding} to swap the role of the two coordinates.
\end{proof}

\begin{lemma}[Composition-product with a kernel]
  \label{thm:fDiv_compProd_right}
  \lean{ProbabilityTheory.fDiv_compProd_right}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_f(\mu \otimes \kappa, \nu \otimes \kappa) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}\leanok
\uses{cor:rnDeriv_compProd_left}
By Corollary~\ref{cor:rnDeriv_compProd_left},
\begin{align*}
D_f(\mu \otimes \kappa, \nu \otimes \kappa)
&= \int_{p} f\left(\frac{d (\mu \otimes \kappa)}{d (\nu \otimes \kappa)}(p)\right) \partial(\nu \otimes \kappa)
\\
&= \int_{p} f\left(\frac{d \mu}{d \nu}(p_X)\right) \partial(\nu \otimes \kappa)
\\
&= \int_x \int_y f\left(\frac{d \mu}{d \nu}(x)\right) \partial \kappa(x) \partial \nu
\\
&= \int_x f\left(\frac{d \mu}{d \nu}(x)\right) \partial \nu
\\
&= D_f(\mu, \nu)
\: .
\end{align*}
\end{proof}

\begin{corollary}
  \label{cor:fDiv_prod_right}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\xi$ be a measure on $\mathcal Y$.
  Then $D_f(\mu \times \xi, \nu \times \xi) = D_f(\mu, \nu)$.
\end{corollary}

\begin{proof}
\uses{thm:fDiv_compProd_right}
Apply Lemma~\ref{thm:fDiv_compProd_right} with $\kappa$ the constant kernel with value $\xi$.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_comp_le_compProd}
  \lean{ProbabilityTheory.fDiv_comp_le_compProd}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two finite measures on a standard Borel space $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  $D_f(\kappa \circ \mu, \eta \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$
\end{lemma}

\begin{proof}\leanok
\uses{thm:fDiv_fst_le}
By definition, $\kappa \circ \mu$ is the marginal of $\mu \otimes \kappa$ (a measure on $\mathcal X \times \mathcal Y$) on $\mathcal Y$, and similarly for the other measure. Hence by Theorem~\ref{thm:fDiv_fst_le}, $D_f(\kappa \circ \mu, \eta \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \eta)$. 
\end{proof}

\begin{theorem}[Conditioning increases f-divergence]
  \label{thm:fDiv_comp_le_condFDiv}
  \lean{ProbabilityTheory.fDiv_comp_left_le}
  \leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a measure on a standard Borel space $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\mu \otimes \kappa, \mu \otimes \eta) = D_f(\kappa, \eta \mid \mu)$
\end{theorem}

\begin{proof}\leanok
\uses{lem:fDiv_comp_le_compProd, lem:fDiv_compProd_left}
By Lemma~\ref{lem:fDiv_comp_le_compProd}, $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\mu \otimes \kappa, \mu \otimes \eta)$. This is equal to $D_f(\kappa, \eta \mid \mu)$ by Lemma~\ref{lem:fDiv_compProd_left}.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:fDiv_data_proc}
  \lean{ProbabilityTheory.fDiv_comp_right_le}
  \leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel, where both $\mathcal X$ and $\mathcal Y$ are standard Borel.
  Then $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}\leanok
\uses{lem:fDiv_comp_le_compProd, thm:fDiv_compProd_right}
By Lemma~\ref{lem:fDiv_comp_le_compProd}, $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \kappa)$. Then the latter is equal to $D_f(\mu, \nu)$ by Lemma~\ref{thm:fDiv_compProd_right}.
\end{proof}

\begin{corollary}
  \label{cor:data_proc_event}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event. Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$. Then $D_f(\mu, \nu) \ge D_f(\mu_E, \nu_E)$.
\end{corollary}

\begin{proof}
\uses{thm:fDiv_data_proc}
Use the deterministic kernel $\kappa : \mathcal X \rightsquigarrow \{0, 1\}$ with $\kappa(x) = \delta_1 \mathbb{I}\{x \in E\} + \delta_0 \mathbb{I}\{x \notin E\}$ in Theorem~\ref{thm:fDiv_data_proc}.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_compProd_prod_eq}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow (\mathcal X \times \mathcal Y)$ be a Markov kernel such that for all $x$, $(\kappa(x))_X = \delta_x$. Then $D_f(\kappa \circ \mu, \kappa \circ \nu) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}
\uses{thm:fDiv_data_proc, thm:fDiv_fst_le}
$D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$ by Theorem~\ref{thm:fDiv_data_proc}.
For the other inequality, remark that $\mu = (\kappa \circ \mu)_X$ (and similarly for $\nu$). Hence by Theorem~\ref{thm:fDiv_fst_le} $D_f(\mu, \nu) \le D_f(\kappa \circ \mu, \kappa \circ \nu)$.
\end{proof}

\section{Convexity}

\begin{theorem}[Joint convexity]
  \label{thm:fDiv_convex}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  The function $(\mu, \nu) \mapsto D_f(\mu, \nu)$ is convex.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_comp_le_condFDiv}
Let $\mu_0, \mu_1, \nu_0, \nu_1$ be four measures. Let $\lambda \in [0,1]$. Let $\xi$ be the probability measure on $\{0,1\}$ with $\xi(\{1\}) = \lambda$.
Let $\kappa$ be the kernel $\{0,1\} \rightsquigarrow \mathcal X$ defined by $\kappa(0) = \mu_0$ and $\kappa(1) = \mu_1$.
Let $\eta$ be the kernel $\{0,1\} \rightsquigarrow \mathcal X$ defined by $\eta(0) = \nu_0$ and $\eta(1) = \nu_1$.
\begin{align*}
D_f(\xi \otimes \kappa, \xi \otimes \eta)
&= D_f(\kappa, \eta \mid \xi)
= (1 - \lambda) D_f(\mu_0, \nu_0) + \lambda D_f(\mu_1, \nu_1)
\: , \\
D_f(\kappa \circ \xi, \eta \circ \xi)
&= D_f((1 - \lambda)\mu_0 + \lambda \mu_1, (1 - \lambda)\nu_0 + \lambda \nu_1)
\: .
\end{align*}
By Theorem~\ref{thm:fDiv_comp_le_condFDiv}, $D_f(\kappa \circ \xi, \eta \circ \xi) \le D_f(\xi \otimes \kappa, \xi \otimes \eta)$.
\end{proof}

\section{Variational representations}

TODO