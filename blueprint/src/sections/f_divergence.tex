\chapter{f-divergences}

\section{Definition and basic properties}

\begin{definition}[f-divergence]
  \label{def:fDiv}
  %\lean{}
  %\leanok
  %\uses{}
  Let $f : \mathbb{R} \to \mathbb{R}$ and let $\mu, \nu$ be two measures on a measurable space $\mathcal X$. The f-divergence between $\mu$ and $\nu$ is
  \begin{align*}
  D_f(\mu, \nu) = \nu\left[x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)\right]
  \end{align*}
  if $x \mapsto f\left(\frac{d \mu}{d \nu}(x)\right)$ is $\nu$-integrable and $+\infty$ otherwise.
\end{definition}

\begin{lemma}
  \label{lem:fDiv_add}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  $D_{f + g}(\mu, \nu) = D_f(\mu, \nu) + D_g(\mu, \nu)$.
\end{lemma}

\begin{proof}
Linearity of the integral.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_self}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  If $f(1) = 0$ then $D_{f}(\mu, \mu) = 0$.
\end{lemma}

\begin{proof}
$\frac{d \mu}{d \mu}(x) = 1$ almost everywhere and $f(1) = 0$.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_mul}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  For all $a \in \mathbb{R}$, $D_{a f}(\mu, \nu) = a D_{f}(\mu, \nu)$.
\end{lemma}

\begin{proof}
Linearity of the integral.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_linear}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  For finite measures $\mu$ and $\nu$ with $\mu(\mathcal X) = \nu(\mathcal X)$, $D_{x - 1}(\mu, \nu) = 0$.
\end{lemma}

\begin{proof}
Computation.
\end{proof}

\begin{lemma}
  \label{lem:fDiv_add_linear}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  For finite measures $\mu$ and $\nu$ with $\mu(\mathcal X) = \nu(\mathcal X)$, for all $a \in \mathbb{R}$, $D_{f + a(x - 1)}(\mu, \nu) = D_{f}(\mu, \nu)$.
\end{lemma}

This means that we can always choose $f \ge 0$ and $f'(1) = 0$ if we want (and $f$ is differentiable at $1$).

\begin{proof}
\uses{lem:fDiv_add, lem:fDiv_mul, lem:fDiv_linear}
Linearity (Lemmas~\ref{lem:fDiv_add} and~\ref{lem:fDiv_mul}), then Lemma~\ref{lem:fDiv_linear}.
\end{proof}

\section{Conditional f-divergence}

\begin{lemma}
  \label{lem:measurable_fDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels from $\mathcal X$ to $\mathcal Y$. Then $x \mapsto D_f(\kappa(x), \eta(x))$ is measurable.
\end{lemma}

\begin{proof}
\end{proof}

\begin{definition}[Conditional f-divergence]
  \label{def:condFDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv, lem:measurable_fDiv}
  Let $f : \mathbb{R} \to \mathbb{R}$, $\mu$ a measure on $\mathcal X$ and $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ two Markov kernels from $\mathcal X$ to $\mathcal Y$. The conditional f-divergence between $\kappa$ and $\eta$ with respect to $\mu$ is
  \begin{align*}
  D_f(\kappa, \eta \mid \mu) = \mu\left[x \mapsto D_f(\kappa(x), \eta(x))\right] \: .
  \end{align*}
\end{definition}

\begin{lemma}
  \label{lem:fDiv_compProd_left}
  %\lean{}
  %\leanok
  \uses{def:condFDiv}
  Let $\mu$ be a measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $D_f(\mu \otimes \kappa, \mu \otimes \eta) = D_f(\kappa, \eta \mid \mu)$.
\end{lemma}

\begin{proof}
\uses{lem:rnDeriv_compProd}
\begin{align*}
D_f(\mu \otimes \kappa, \mu \otimes \eta)
&= (\mu \otimes \kappa)\left[ x \mapsto \frac{d (\mu \otimes \kappa)}{d (\mu \otimes \eta)}(x) \right]
\end{align*}
TODO
\end{proof}

\section{Data-processing inequality}

\begin{theorem}[Marginals]
  \label{thm:fDiv_fst_le}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu$ and $\nu$ be two measures on $\mathcal X \times \mathcal Y$, and $\mu_X, \nu_X$ be their marginals on $\mathcal X$.
  Then $D_f(\mu_X, \nu_X) \le D_f(\mu, \nu)$.
  Similarly, for $\mu_Y, \nu_Y$ be the marginals on $\mathcal Y$, $D_f(\mu_Y, \nu_Y) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}
\end{proof}

\begin{lemma}[Composition-product with a kernel]
  \label{thm:fDiv_compProd_right}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_f(\mu \otimes \kappa, \nu \otimes \kappa) = D_f(\mu, \nu)$.
\end{lemma}

\begin{proof}
\end{proof}

\begin{corollary}
  \label{cor:fDiv_prod_right}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\xi$ be a measure on $\mathcal Y$.
  Then $D_f(\mu \times \xi, \nu \times \xi) = D_f(\mu, \nu)$.
\end{corollary}

\begin{proof}
\uses{thm:fDiv_compProd_right}
Apply Lemma~\ref{thm:fDiv_compProd_right} with $\kappa$ the constant kernel with value $\xi$.
\end{proof}

\begin{theorem}[Conditioning increases f-divergence]
  \label{thm:fDiv_comp_le_condFDiv}
  %\lean{}
  %\leanok
  \uses{def:fDiv, def:condFDiv}
  Let $\mu$ be a measure on $\mathcal X$ and let $\kappa, \eta : \mathcal X \rightsquigarrow \mathcal Y$ be two Markov kernels.
  Then $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\kappa, \eta \mid \mu)$
\end{theorem}

\begin{proof}
\uses{thm:fDiv_fst_le, lem:fDiv_compProd_left}
By definition, $\kappa \circ \mu$ is the marginal of $\mu \otimes \kappa$ (a measure on $\mathcal X \times \mathcal Y$) on $\mathcal Y$. Hence by Theorem~\ref{thm:fDiv_fst_le}, $D_f(\kappa \circ \mu, \eta \circ \mu) \le D_f(\mu \otimes \kappa, \mu \otimes \eta)$. This is equal to $D_f(\kappa, \eta \mid \mu)$ by Lemma~\ref{lem:fDiv_compProd_left}.
\end{proof}

\begin{theorem}[Data-processing]
  \label{thm:fDiv_data_proc}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $\kappa : \mathcal X \rightsquigarrow \mathcal Y$ be a Markov kernel.
  Then $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu, \nu)$.
\end{theorem}

\begin{proof}
\uses{thm:fDiv_fst_le, thm:fDiv_compProd_right}
By definition, $\kappa \circ \mu$ is the marginal of $\mu \otimes \kappa$ (a measure on $\mathcal X \times \mathcal Y$) on $\mathcal Y$. Hence by Theorem~\ref{thm:fDiv_fst_le}, $D_f(\kappa \circ \mu, \kappa \circ \nu) \le D_f(\mu \otimes \kappa, \nu \otimes \kappa)$. Then the latter is equal to $D_f(\mu, \nu)$ by Lemma~\ref{thm:fDiv_compProd_right}.
\end{proof}

\begin{corollary}
  \label{cor:data_proc_event}
  %\lean{}
  %\leanok
  \uses{def:fDiv}
  Let $\mu, \nu$ be two measures on $\mathcal X$ and let $E$ be an event. Let $\mu_E$ and $\nu_E$ be the two Bernoulli distributions with respective means $\mu(E)$ and $\nu(E)$. Then $D_f(\mu, \nu) \ge D_f(\mu_E, \nu_E)$.
\end{corollary}

\begin{proof}
\uses{thm:fDiv_data_proc}
Use the deterministic kernel $\kappa : \mathcal X \rightsquigarrow \{0, 1\}$ with $\kappa(x) = \delta_1 \mathbb{I}\{x \in E\} + \delta_0 \mathbb{I}\{x \notin E\}$ in Theorem~\ref{thm:fDiv_data_proc}.
\end{proof}


\section{Variational representations}

TODO